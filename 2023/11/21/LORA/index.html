<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LoRA | Hello</title><meta name="author" content="sagonimikakokomi"><meta name="copyright" content="sagonimikakokomi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LoRA一、全参数微调 我们知道，微调的含义，就是把已经训练好的模型（pretrained model）拿来，给它吃特定的下游任务数据，使得模型在预训练权重上继续训练，直至满足下游任务性能标准。预训练模型就像一个特征提取器，能够基于先前训练数据中学到的经验，为我们提取有效的特征，大大提升下游任务的训练效果和收敛速度。 全量微调指的是，在下游任务的训练中，对预训练模型的每一个参数都做更新">
<meta property="og:type" content="article">
<meta property="og:title" content="LoRA">
<meta property="og:url" content="https://kokomisukisuki.github.io/2023/11/21/LORA/index.html">
<meta property="og:site_name" content="Hello">
<meta property="og:description" content="LoRA一、全参数微调 我们知道，微调的含义，就是把已经训练好的模型（pretrained model）拿来，给它吃特定的下游任务数据，使得模型在预训练权重上继续训练，直至满足下游任务性能标准。预训练模型就像一个特征提取器，能够基于先前训练数据中学到的经验，为我们提取有效的特征，大大提升下游任务的训练效果和收敛速度。 全量微调指的是，在下游任务的训练中，对预训练模型的每一个参数都做更新">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.helloimg.com/images/2023/10/03/oH3BLh.jpg">
<meta property="article:published_time" content="2023-11-20T16:00:00.000Z">
<meta property="article:modified_time" content="2023-11-21T14:00:51.257Z">
<meta property="article:author" content="sagonimikakokomi">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.helloimg.com/images/2023/10/03/oH3BLh.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kokomisukisuki.github.io/2023/11/21/LORA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"UNK1PFAIM3","apiKey":"82e2706c98842df0554f4f4c562132c7","indexName":"kokomisuki","hits":{"per_page":20},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: sagonimikakokomi","link":"链接: ","source":"来源: Hello","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LoRA',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-21 22:00:51'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/color.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="/css/diy.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.css"/><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://ooo.0x0.ooo/2023/10/02/OnIWmL.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">218</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://www.helloimg.com/images/2023/10/03/oHxfDY.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Hello"><span class="site-name">Hello</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LoRA</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-11-20T16:00:00.000Z" title="发表于 2023-11-21 00:00:00">2023-11-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-11-21T14:00:51.257Z" title="更新于 2023-11-21 22:00:51">2023-11-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer"/>





<h1 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h1><h2 id="一、全参数微调"><a href="#一、全参数微调" class="headerlink" title="一、全参数微调"></a>一、全参数微调</h2><p><img src="https://pic4.zhimg.com/80/v2-a73e3481477a811f6beb5689b72a5dd7_720w.webp" alt=""></p>
<p>我们知道，微调的含义，就是把已经训练好的模型（pretrained model）拿来，给它吃特定的下游任务数据，使得模型在预训练权重上继续训练，直至满足下游任务性能标准。预训练模型就像一个<strong>特征提取器</strong>，能够基于先前训练数据中学到的经验，为我们提取有效的特征，大大提升下游任务的训练效果和收敛速度。</p>
<p><strong>全量微调</strong>指的是，在下游任务的训练中，对预训练模型的每一个参数都做更新。例如图中，给出了Transformer的Q/K/V矩阵的全量微调示例，对每个矩阵来说，在微调时，其<code>d*d</code>个参数，都必须参与更新。</p>
<p>全量微调的显著缺点是，<strong>训练代价昂贵</strong>。例如GPT3的参数量有175B，我等单卡贵族只能望而却步，更不要提在微调中发现有bug时的覆水难收。同时，由于模型在预训练阶段已经吃了足够多的数据，收获了足够的经验，因此我<strong>只要想办法给模型增加一个额外知识模块，让这个小模块去适配我的下游任务，模型主体保持不变（freeze）即可</strong>。</p>
<p>那这样的知识小模块，具体要怎么添加呢？</p>
<h2 id="二、Adapter-Tuning与Prefix-Tuning"><a href="#二、Adapter-Tuning与Prefix-Tuning" class="headerlink" title="二、Adapter Tuning与Prefix Tuning"></a>二、Adapter Tuning与Prefix Tuning</h2><p>我们来看在LoRA出现前，两种主流的局部微调办法：<strong>Adapter Tuning与Prefix Tuning</strong>。这也是LoRA的原始论文中，重点比对的两种微调方式。</p>
<h3 id="2-1-Adapter-Tuning"><a href="#2-1-Adapter-Tuning" class="headerlink" title="2.1 Adapter Tuning"></a>2.1 Adapter Tuning</h3><p><img src="https://pic3.zhimg.com/80/v2-47bee3d3a36af9608147a7b41f672a96_720w.webp" alt=""></p>
<p>Adapter Tuning的方法有很多种，这里我们举出Houlsby et al. ,2019提出的方法，这也是LoRA论文中提及这项技术时所引用的第一篇文章。</p>
<p>图例中的左边是一层Transformer Layer结构，其中的Adapter就是我们说的“额外知识模块”；右边是Adatper的具体结构。<strong>在微调时，除了Adapter的部分，其余的参数都是被冻住的（freeze）</strong>，这样我们就能有效降低训练的代价。Adapter的内部架构不是本文所述的重点，这里我们就不再介绍了。</p>
<p>但这样的设计架构存在一个<strong>显著劣势</strong>：<strong>添加了Adapter后，模型整体的层数变深，会增加训练速度和推理速度</strong>，原因是：</p>
<ul>
<li>需要耗费额外的运算量在Adapter上</li>
<li>当我们采用并行训练时（例如Transformer架构常用的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/622212228">张量模型并行</a>），Adapter层会产生额外的通讯量，增加通讯时间</li>
</ul>
<h3 id="2-2-Prefix-Tuning"><a href="#2-2-Prefix-Tuning" class="headerlink" title="2.2 Prefix Tuning"></a>2.2 Prefix Tuning</h3><p><img src="https://pic2.zhimg.com/80/v2-13dee0b8f314bc7807c076af90365be9_720w.webp" alt=""></p>
<p>Prefix Tuning的方法也有很多种，这里我们选取Li&amp;Liang,2021这一篇进行简述。在这篇中，作者通过对输入数据增加前缀（prefix）来做微调。<strong>当然，prefix也可以不止加载输入层，还可以加在Transformer Layer输出的中间层</strong>，感兴趣的朋友可以查找论文自行研究。</p>
<p>如图所示，对于<strong>GPT这样的生成式模型</strong>，在输入序列的最前面加入prefix token，图例中加入2个prefix token，在实际应用中，prefix token的个数是个超参，可以根据模型实际微调效果进行调整。对于<strong>BART这样的Encoder-Decoder架构模型</strong>，则在x和y的前面同时添加prefix token。<strong>在后续微调中，我们只需要冻住模型其余部分，单独训练prefix token相关的参数即可，每个下游任务都可以单独训练一套prefix token。</strong></p>
<p><strong>那么prefix的含义是什么呢？</strong>prefix的作用是引导模型提取x相关的信息，进而更好地生成y。例如，我们要做一个<strong>summarization</strong>的任务，那么经过微调后，prefix就能领悟到当前要做的是个“总结形式”的任务，然后引导模型去x中提炼关键信息；如果我们要做一个<strong>情感分类</strong>的任务，prefix就能引导模型去提炼出x中和情感相关的语义信息，以此类推。这样的解释可能不那么严谨，但大家可以大致体会一下prefix的作用。</p>
<p>Prefix Tuning虽然看起来方便，但也存在以下两个显著劣势；</p>
<ul>
<li>较难训练，且模型的效果并不严格随prefix参数量的增加而上升，这点在原始论文中也有指出</li>
<li>会使得输入层有效信息长度减少。为了节省计算量和显存，我们一般会固定输入数据长度。增加了prefix之后，留给原始文字数据的空间就少了，因此可能会降低原始文字中prompt的表达能力。</li>
</ul>
<h2 id="三、什么是LoRA"><a href="#三、什么是LoRA" class="headerlink" title="三、什么是LoRA"></a>三、什么是LoRA</h2><p>总结一下，<strong>全参数微调太贵，Adapter Tuning存在训练和推理延迟，Prefix Tuning难训且会减少原始训练数据中的有效文字长度</strong>，那是否有一种微调办法，能改善这些不足呢？</p>
<p>在这样动机的驱动下，作者提出了<strong>LoRA（Low-Rank Adaptation，低秩适配器）这样一种微调方法</strong>。我们先抛开对“低秩”、“适配器”这样抽象词语的解释，我们先来看LoRA长什么样，要怎么用。在下一节中，我们再来详细解释“低秩”作用的原理。</p>
<p><img src="https://pic2.zhimg.com/80/v2-d8b60ee764df0e7fa6ea129ed158f475_720w.webp" alt=""></p>
<p><strong>图中左侧表示“全参数finetune”的场景</strong>。我们将参数分成了两个部分：</p>
<p>$\begin{array}{l}\bullet&amp;W\in\mathbb{R}^{d<em>d}:\text{预训练权重}\\\bullet&amp;\Delta W\in\mathbb{R}^{d</em>d}:\text{ finetune增量权重}\end{array}$</p>
<p>之所以这么拆分，是因为<strong>全参数finetune可以理解成“冻住的预训练权重” + “微调过程中产生的权重更新量”</strong>。<br>设输入为 x ，输出为 ℎ ，则有：</p>
<p>$h=Wx+\Delta Wx$</p>
<p><strong>图中右侧表示“LoRA finetune”的场景。在LoRA中，我们用矩阵A和B来近似表达</strong> ΔW：</p>
<p>$\begin{array}{l}\bullet&amp;A\in\mathbb{R}^{r<em>d}:\text{ 低秩矩阵 }A\text{,其中 }r\text{被称为‘秩’}\text{ ,对 }A\text{ 用高斯初始化。}\\\bullet&amp;B\in\mathbb{R}^{d</em>r}:\text{ 低秩矩阵 }B\text{,对B采用零初始化。}\end{array}$</p>
<p>经过这样一番拆分，<strong>我们将</strong> ΔW <strong>改写成</strong> ΔW=B <strong>的形式，使得微调参数量从<code>d*d</code>降低至<code>2*r*d</code>，同时不改变输出数据的维度</strong>，即在LoRA下我们有:<br>ℎ=Wx+BAx</p>
<p>另外，<strong>在原论文中提到过对于两个低秩矩阵，会用超参</strong> $\alpha$ <strong>（一个常数）来做调整</strong>，但没有说明这个超参的作用位置。在读完LoRA的源码后，我发现<strong>这个超参是作为scaling rate直接和低秩矩阵相乘的</strong>，也就是最终的输出为：</p>
<p>$h=Wx+\frac{\alpha}{r}BAx$</p>
<p>在实操中，一般取 $\alpha$≥r ，例如在LoRA源码对GPT2微调，做NLG任务时，就取 $\alpha$=32,r=4 。<strong>我们会在后文详细介绍这个scaling rate的作用，以及“秩”的具体含义。</strong></p>
<p><strong>A和B的初始化方法</strong></p>
<p>需要注意的是，这里对 A 采用高斯初始化，对 B 采用零初始化的目的是，让训练刚开始时 B 的值为0，这样不会给模型带来额外的噪声。那么你可能想问，<strong>那我对</strong> A<strong>做零初始化，对</strong>B <strong>做高斯初始化行不行呢？反正看起来只要让</strong> BA <strong>初始化为0就行？</strong></p>
<p>针对这个问题，我在github issue上找到了LoRA一作的回答：</p>
<p><img src="https://pic1.zhimg.com/80/v2-7d30ca2e1830f215d3d6d0e7f7737b24_720w.webp" alt=""></p>
<p>简单来说，当前作者还没有发现转换 B,A 初始化方式产生的显著区别，只要这两者中任意一者为0，另一者不为0即可。</p>
<h3 id="3-2-LoRA的训练和推理过程"><a href="#3-2-LoRA的训练和推理过程" class="headerlink" title="3.2 LoRA的训练和推理过程"></a>3.2 LoRA的训练和推理过程</h3><p>在3.1中，<strong>我们介绍了LoRA的整体架构：在原始预训练矩阵的旁路上，用低秩矩阵A和B来近似替代增量更新</strong> ΔW 。你可以在你想要的模型层上做这样的操作，比如Transformer中的 Wq,Wk,Wv,Wo ,MLP层的权重、甚至是Embedding部分的权重。在LoRA原始论文中，只对Attention部分的参数做了低秩适配，但在实际操作中，我们可以灵活根据需要设置实验方案，找到最佳的适配方案（有钱万事通）。</p>
<p><strong>3.2.1 训练</strong></p>
<p>在<strong>训练过程</strong>中，我们固定住预训练权重 W ，只对低秩矩阵 A 和 B 进行训练。<strong>在保存权重时，我们只需保存低秩矩阵的部分即可</strong>。按照LoRA论文中的统计，这样的操作使得在微调GPT3 175B时，显存消耗从1.2TB降至350GB；当r=4时，最终保存的模型从350GB降至35MB，极大降低了训练的开销。</p>
<p>关于训练部分，我们再来看一个有趣的问题：总体上来看，LoRA对显存的节约是显著的，但是在训练的每一时刻，LoRA都能做到节省显存吗？</p>
<p>计算其梯度，发现因此对LoRA来说，<strong>这一层的峰值显存，和全量微调基本是一致的</strong>。</p>
<p><strong>但是为什么LoRA又能从整体上降低显存使用呢</strong>，因为：</p>
<ul>
<li>LoRA并不是作用在模型的每一层，例如论文里的LoRA只作用在attention部分</li>
<li>LoRA虽然会导致某一层的峰值显存高于全量微调，但计算完梯度后，这个中间结果就可以被清掉了，不会一致保存</li>
<li>当待训练权重从<code>d*d</code>降为<code>2*r*d</code>时，需要保存的optimizer states也减少了（那可是fp32）。</li>
</ul>
<p><strong>3.2.2 推理</strong></p>
<p>在<strong>推理过程</strong>中，<strong>我们按照</strong> $W=W+\frac{\alpha}{r}BA$ <strong>的方式，合并低秩矩阵和预训练权重，然后正常做forward推理。这样我们完全不会更改模型的架构，因此不会像Adapter Tuning一样产生推理上的延时</strong>。下图展示了论文中的实验效果，推理时长的单位是milliseconds，可以发现，LoRA的推理速度显著高于Adapter Tuning。</p>
<p><img src="https://pic2.zhimg.com/80/v2-6f68e7c6bf7f1b72c20606e9fa4ba57d_720w.webp" alt=""></p>
<p>在<strong>切换不同下游任务</strong>时，我们可以灵活从 W 中移除低秩权重的部分。例如我们先做下游任务A，做完后通过 $W=W+\frac{\alpha}{r}BA$  合并权重，并单独保留低秩权重 A，B。当我们切换到下游任务B时，我们可以通过从 W 中减去低秩权重部分，然后再开启新的LoRA微调。也就是说，<strong>每个下游任务，都可以有自己的一套低秩权重</strong>。</p>
<p><strong>你可能想问，在每次微调结束后，我一定要把低秩权重合进</strong>W <strong>中吗？我可以将“预训练权重”和“低秩权重”分开存储吗？</strong>当然没问题啦，LoRA是很灵活的，你完全可以根据自身需要，改写代码，决定权重的保存方式，只要掌握一个核心原则：不管是合还是不合，你总有办法能区分出预训练和LoRA的部分，就行。在源码解读篇中，我们会再详细来看这点。</p>
<h2 id="四、LoRA低秩适配的原理"><a href="#四、LoRA低秩适配的原理" class="headerlink" title="四、LoRA低秩适配的原理"></a>四、LoRA低秩适配的原理</h2><p>在前文中，我们曾反复提到“秩”的概念，并说明LoRA的秩即为超参 r ，同时，我们也不断强调 BA 是 ΔW 的<strong>近似。</strong>在这一节中，<strong>我们将具象化地来看看“秩”，并说明为何是“近似”，在了解这些后，我们就能来解读超参</strong> $\alpha$ <strong>的作用，并掌握一定的炼丹感觉了。</strong></p>
<h3 id="4-1-什么是秩"><a href="#4-1-什么是秩" class="headerlink" title="4.1 什么是秩"></a>4.1 什么是秩</h3><p>秩的计算与线性代数中一致。</p>
<p>可以说，<strong>秩表示的是矩阵的信息量</strong>。如果矩阵中的某一维，总可以通过其余维度线性推导而来，那么对模型来说，这一维的信息是冗余的，是重复表达的。分为秩亏（rank deficient）与满秩（full rank）。</p>
<p>有了对秩的这层认识，我们自然会想到，<strong>全参数微调中的增量权重</strong> ΔW <strong>可能也存在冗余的信息，因此我们并不需要用完整的<code>d*d</code> 尺寸来表示它</strong>。那么，<strong>我们要如何找出</strong>ΔW中真正有用的特征维度呢？SVD分解（奇异值分解），可以帮我们解决这个问题</p>
<h3 id="4-2-SVD分解"><a href="#4-2-SVD分解" class="headerlink" title="4.2 SVD分解"></a>4.2 SVD分解</h3><p><img src="https://pic3.zhimg.com/80/v2-f97f902c17850d2d94c490bae5d89b4a_720w.webp" alt=""></p>
<p>$\begin{aligned}<br>&amp;\text{如图,矩阵 M 是我们需要做信息量检查的矩阵。假设在输入数据的特征空间中,存在一组正交的单位向量 } \\<br>&amp;\overrightarrow{\textbf{}v_1},\overrightarrow{v_2}\text{ ,经过 }M\text{ 的变换后,它们变成另一组正交向量 }\sigma_1\overrightarrow{u_1},\sigma_2\overrightarrow{u_2}\text{,其中 }\overrightarrow{u_1},\overrightarrow{u_2}\textbf{ 也} \\<br>&amp;\textbf{是一组正交的单位向量,}\sigma_1,\sigma_2\text{ 分别表示对应方向上的模。上面这一顿变幻,可以写成}: \\<br>&amp;M[\overrightarrow{v_{1}},\overrightarrow{v_{2}}]=[\sigma_{1}\overrightarrow{u_{1}},\sigma_{2}\overrightarrow{u_{2}}] \\<br>&amp;\text{稍加改写,就有:} \\<br>&amp;\left.M=\left[\begin{matrix}\overrightarrow{u_1}&amp;\overrightarrow{u_2}\end{matrix}\right.\right]\left[\begin{matrix}\sigma_1&amp;0\\0&amp;\sigma_2\end{matrix}\right]\left[\begin{matrix}\overrightarrow{v_1}\\\overrightarrow{v_2}\end{matrix}\right] \\<br>&amp;\text{不难发现,}\sigma_1,\sigma_2\textbf{ 中隐含了对 “信息量”的提示。在本例中 }v\text{ 经过 }M\text{ 的转换投射到 }u\text{ 上时}, \\<br>&amp;M\text{ 强调了在1方向上蕴含的信息。}<br>\end{aligned}$</p>
<p>现在再宽泛一些，如果我们能找到这样的一组 v 和 u ，并令 $\sigma$矩阵的值从大到小进行排列，那么我们不就能对 M 进行拆解，同时在拆解过程中，找出 M 所强调的那些特征方向了吗？也就是说：</p>
<p>$M=U\Sigma V^{T}$</p>
<p><em>与线性代数当中的奇异值分解一致，高维转换成低维表示</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line"><span class="comment"># n：输入数据维度</span></span><br><span class="line"><span class="comment"># m：输出数据维度</span></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line">m = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line"><span class="comment"># 随机初始化权重W</span></span><br><span class="line"><span class="comment"># 之所以这样初始化，是为了让W不要满秩，</span></span><br><span class="line"><span class="comment"># 这样才有低秩分解的意义</span></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line">nr = <span class="number">10</span></span><br><span class="line">mr = <span class="number">2</span></span><br><span class="line">W = torch.randn(nr,mr)@torch.randn(mr,nr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line"><span class="comment"># 随机初始化输入数据x</span></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line">x = torch.randn(n)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line"><span class="comment"># 计算Wx</span></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line">y = W@x</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始权重W计算出的y值为:\n&quot;</span>, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line"><span class="comment"># 计算W的秩</span></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line">r= np.linalg.matrix_rank(W)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W的秩为: &quot;</span>, r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line"><span class="comment"># 对W做SVD分解</span></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line">U, S, V = torch.svd(W)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line"><span class="comment"># 根据SVD分解结果，</span></span><br><span class="line"><span class="comment"># 计算低秩矩阵A和B</span></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line">U_r = U[:, :r]</span><br><span class="line">S_r = torch.diag(S[:r])</span><br><span class="line">V_r = V[:,:r].t()</span><br><span class="line"></span><br><span class="line">B = U_r@S_r <span class="comment"># shape = (d, r)</span></span><br><span class="line">A = V_r     <span class="comment"># shape = (r, d)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line"><span class="comment"># 计算y_prime = BAx</span></span><br><span class="line"><span class="comment"># ------------------------------------</span></span><br><span class="line">y_prime = B@A@x</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;SVD分解W后计算出的y值为:\n&quot;</span>, y_prime)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始权重W的参数量为: &quot;</span>, W.shape[<span class="number">0</span>]*W.shape[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;低秩适配后权重B和A的参数量为: &quot;</span>, A.shape[<span class="number">0</span>]*A.shape[<span class="number">1</span>] + B.shape[<span class="number">0</span>]*B.shape[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">原始权重W计算出的y值为:</span><br><span class="line"> tensor([ <span class="number">3.3896</span>,  <span class="number">1.0296</span>,  <span class="number">1.5606</span>, -<span class="number">2.3891</span>, -<span class="number">0.4213</span>, -<span class="number">2.4668</span>, -<span class="number">4.4379</span>, -<span class="number">0.0375</span>,</span><br><span class="line">        -<span class="number">3.2790</span>, -<span class="number">2.9361</span>])</span><br><span class="line">W的秩为:  <span class="number">2</span></span><br><span class="line">SVD分解W后计算出的y值为:</span><br><span class="line"> tensor([ <span class="number">3.3896</span>,  <span class="number">1.0296</span>,  <span class="number">1.5606</span>, -<span class="number">2.3891</span>, -<span class="number">0.4213</span>, -<span class="number">2.4668</span>, -<span class="number">4.4379</span>, -<span class="number">0.0375</span>,</span><br><span class="line">        -<span class="number">3.2790</span>, -<span class="number">2.9361</span>])</span><br><span class="line">原始权重W的参数量为:  <span class="number">100</span></span><br><span class="line">低秩适配后权重B和A的参数量为:  <span class="number">40</span></span><br></pre></td></tr></table></figure>
<p><strong>参数量变少了，但并不影响最终输出的结果</strong>。</p>
<h3 id="4-3-LoRA低秩适配"><a href="#4-3-LoRA低秩适配" class="headerlink" title="4.3 LoRA低秩适配"></a>4.3 LoRA低秩适配</h3><p>好，那既然SVD分解这么有效，那我直接对 ΔW 做SVD，找到对应的低秩矩阵 A,B ，不就大功告成了吗？<br><strong>想法虽然好，但困难是明显的：能直接做SVD的前提是</strong>ΔW<strong>是确定的</strong>，而现实中ΔW作为全参数微调中的权重增量，如果你不全参数微调一遍，又怎么能知道ΔW长什么样呢？而如果你做了全量微调，那还要低秩适配做什么呢？</p>
<p>欸，你可能又想：那我能不能对预训练权重W 做SVD呢，因为 W 是确定的呀。<br><strong>想法虽然好，但逻辑是不合理的：我们说过，微调的目的是给模型注入和下游任务相关的领域新知识</strong>。也就是说，ΔW<strong>和</strong> W<strong>的表达含义是不同的，前者是新知识，后者是旧知识</strong>，我们的目的是要去新知识中拆解信息量丰富的维度。</p>
<p>好，<strong>那既然通过数学方法直接做SVD行不通，那就让模型自己去学怎么做SVD吧！</strong>因此LoRA最终的低秩适配策略是：我把秩 r 当成一个超参，再让模型自己去学低秩矩阵 A,B .</p>
<h3 id="4-4-超参-alpha"><a href="#4-4-超参-alpha" class="headerlink" title="4.4 超参 $\alpha$"></a>4.4 超参 $\alpha$</h3><p>我们先来看论文对$\alpha$的解释：</p>
<p><img src="https://pic1.zhimg.com/80/v2-b5a98033309f587201a1ba649090f058_720w.webp" alt=""></p>
<p>这段话大致意思是说，在我们采用Adam做优化器时，调整$\alpha$的作用就相当于调整learning rate。一般而言，我们把$\alpha$设置为我们第一次做实验时设置的r ，然后就把$\alpha$固定下来，之后只调整 r 即可，这样做的好处是当我们尝试不同的 r 时，我们不需要再去调整别的超参了。</p>
<p>首先，回顾一下我们的输出计算方法为：</p>
<p>其中， W 表示预训练权重（<strong>旧知识</strong>），$\frac{\alpha}{r}BA$表示增量权重 ΔW的近似（<strong>新知识</strong>）。理论上说，当 r <strong>较小时，我们提取的是</strong> ΔW<strong>中信息含量最丰富的维度，此时信息精炼，但不全面；当</strong> r <strong>较大时，我们的低秩近似越逼近</strong>ΔW<strong>，此时信息更加全面，但带来的噪声也越多（含有很多冗余无效的信息）</strong>。</p>
<p>基于这个猜想，当我们第一次做实验时，我们会尽量把 r 调得大些，例如32、64，并假设在这个秩下，低秩权重已经非常近似 ΔW 了，因此这时我们设置 $\alpha$=r ，意味着我们假定LoRA低秩微调的效果和全参数微调持平。</p>
<p>那么接下来，我们肯定就要往小的 r进行尝试了。这时我们把 $\alpha$ 固定住，意味着随着 r 的减小， $\frac{\alpha}{r}$ 会越来越大，我们这样做的原因是：</p>
<ul>
<li><strong>当</strong>r<strong>越小时，低秩矩阵表示的信息精炼，但不全面。我们通过调大</strong>$\frac{\alpha}{r}$<strong>，来放大forward过程中新知识对模型的影响。</strong></li>
<li><strong>当</strong>r<strong>越小时，低秩矩阵表示的信息精炼，噪声/冗余信息少，此时梯度下降的方向也更加确信，所以我们可以通过调大</strong>$\frac{\alpha}{r}$<strong>，适当增加梯度下降的步伐，也就相当于调整learning rate了。</strong></li>
</ul>
<h2 id="五、LoRA实验：验证低秩矩阵的有效性"><a href="#五、LoRA实验：验证低秩矩阵的有效性" class="headerlink" title="五、LoRA实验：验证低秩矩阵的有效性"></a>五、LoRA实验：验证低秩矩阵的有效性</h2><h3 id="5-1-整体效果"><a href="#5-1-整体效果" class="headerlink" title="5.1 整体效果"></a>5.1 整体效果</h3><p><img src="https://pic3.zhimg.com/80/v2-7bca7b8537500fd5edb8c9745ca6adae_720w.webp" alt=""></p>
<p>首先，作者将LoRA和其余微调方法（全参数微调，Adatper Tuning等）做了比较。纵列表示不同的微调模型，横列表示不同的数据集，加粗部分表示最好的效果指标。可以发现，无论是在各个数据集微调准确率指标上，还是在最后平均微调准确率指标上（Avg.），LoRA都取得了不错的表现，而且它可训练的参数量也非常小。</p>
<h3 id="5-2-低秩矩阵信息量验证"><a href="#5-2-低秩矩阵信息量验证" class="headerlink" title="5.2 低秩矩阵信息量验证"></a>5.2 低秩矩阵信息量验证</h3><p>我们前面说过，当 r 越小时，低秩矩阵所含的信息越精炼，但同时也可能越不全面。那么到底 r 要取多少才合适呢？</p>
<p><strong>5.2.1 直接验证不同r值下的微调效果</strong></p>
<p>尽管理论上我们可以在模型的任意一层嵌入低秩适配器（比如Embedding， Attention，MLP等），但LoRA中只选咋在Attention层嵌入，并做了相关实验（论文中也鼓励读者可以多做别的尝试），我们来看下Attention层的实验效果：</p>
<p><img src="https://pic3.zhimg.com/80/v2-5c2a45e10b32cb191f1fd2ed389f3c7a_720w.webp" alt=""></p>
<p><code>WikiSQL</code> 和<code>MultiNLI</code>是用于微调的数据集，Weight Type指明在Attention的哪一部分做了低秩适配。可以发现， r=4,8 于 r=64 的效果几乎持平，甚至还略优于 r=64 。这更加说明了“低秩”的有效性。<strong>为了更具象化地验证这一点，我们进一步来看</strong> r=8 <strong>和</strong> r=64 <strong>这两个低秩空间的相交程度</strong>。</p>
<p><strong>5.2.2 不同低秩空间的相交程度</strong></p>
<p>$\phi(A_{r=8},A_{r=64},i,j)=\frac{||U_{Ar=8}^{i\top}U_{A_r=64}^{j}||_{F}^{2}}{\min(i,j)}\in[0,1]$</p>
<p>从矩阵$A_{r=8}与A_{r=64}$当中提取出富含信息最多的几个维度，并对于两者的维度信息进行确定相交指标，相交指标也被称为”<strong>Grassmann distance</strong>“。(其中用到矩阵的特征值分解等)</p>
<p><img src="https://pic3.zhimg.com/80/v2-ac4cb828436f7d85d9ca394783815ed2_720w.webp" alt=""></p>
<p><strong>5.2.3 不同层的r值设置</strong></p>
<p><strong>5.2.4 预训练权重 VS 微调权重</strong></p>
<h1 id="LoRA源码"><a href="#LoRA源码" class="headerlink" title="LoRA源码"></a>LoRA源码</h1><h1 id="AdaLoRA"><a href="#AdaLoRA" class="headerlink" title="AdaLoRA"></a>AdaLoRA</h1><p><strong>AdaLoRA的总体改进目标</strong>：<br><strong>找到一种办法，让模型在微调过程中，去学习每个模块参数对训练结果（以loss衡量）的重要性。然后，根据重要性，动态地调整不同模块的秩（r）。</strong></p>
<p>该策略叫<strong>参数预算（parameter budget）</strong></p>
<p>（感觉论文难度不高，使用的技巧也主要是特征值分解换汤不换药，有时间还是自己推吧）</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kokomisukisuki.github.io">sagonimikakokomi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kokomisukisuki.github.io/2023/11/21/LORA/">https://kokomisukisuki.github.io/2023/11/21/LORA/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kokomisukisuki.github.io" target="_blank">Hello</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post_share"><div class="social-share" data-image="https://www.helloimg.com/images/2023/10/03/oH3BLh.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/11/21/google%E6%90%9C%E7%B4%A2/" title="google搜索"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oHqYBm.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">google搜索</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/11/21/postman/" title="postman"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oHEpr6.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-21</div><div class="title">postman</div></div></a></div><div><a href="/2023/11/18/transformer/" title="transformer"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oHX5TE.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-18</div><div class="title">transformer</div></div></a></div><div><a href="/2023/11/21/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C/" title="流水线并行（Pipeline Parallelism）"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oH3BLh.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-21</div><div class="title">流水线并行（Pipeline Parallelism）</div></div></a></div><div><a href="/2023/11/18/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%AD%A5/" title="强化学习"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oH3BLh.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-18</div><div class="title">强化学习</div></div></a></div><div><a href="/2023/11/18/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%EF%BC%88stable%20diffusion%EF%BC%89/" title="DDPM扩散模型（stable diffusion）"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oHJc2r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-18</div><div class="title">DDPM扩散模型（stable diffusion）</div></div></a></div><div><a href="/2023/11/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%EF%BC%88GAN%EF%BC%89/" title="生成对抗网络（GAN）"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oH3BLh.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-18</div><div class="title">生成对抗网络（GAN）</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LoRA"><span class="toc-text">LoRA</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83"><span class="toc-text">一、全参数微调</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Adapter-Tuning%E4%B8%8EPrefix-Tuning"><span class="toc-text">二、Adapter Tuning与Prefix Tuning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Adapter-Tuning"><span class="toc-text">2.1 Adapter Tuning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Prefix-Tuning"><span class="toc-text">2.2 Prefix Tuning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AFLoRA"><span class="toc-text">三、什么是LoRA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-LoRA%E7%9A%84%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B"><span class="toc-text">3.2 LoRA的训练和推理过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81LoRA%E4%BD%8E%E7%A7%A9%E9%80%82%E9%85%8D%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-text">四、LoRA低秩适配的原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E4%BB%80%E4%B9%88%E6%98%AF%E7%A7%A9"><span class="toc-text">4.1 什么是秩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-SVD%E5%88%86%E8%A7%A3"><span class="toc-text">4.2 SVD分解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-LoRA%E4%BD%8E%E7%A7%A9%E9%80%82%E9%85%8D"><span class="toc-text">4.3 LoRA低秩适配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E8%B6%85%E5%8F%82-alpha"><span class="toc-text">4.4 超参 $\alpha$</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81LoRA%E5%AE%9E%E9%AA%8C%EF%BC%9A%E9%AA%8C%E8%AF%81%E4%BD%8E%E7%A7%A9%E7%9F%A9%E9%98%B5%E7%9A%84%E6%9C%89%E6%95%88%E6%80%A7"><span class="toc-text">五、LoRA实验：验证低秩矩阵的有效性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%95%B4%E4%BD%93%E6%95%88%E6%9E%9C"><span class="toc-text">5.1 整体效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BD%8E%E7%A7%A9%E7%9F%A9%E9%98%B5%E4%BF%A1%E6%81%AF%E9%87%8F%E9%AA%8C%E8%AF%81"><span class="toc-text">5.2 低秩矩阵信息量验证</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LoRA%E6%BA%90%E7%A0%81"><span class="toc-text">LoRA源码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AdaLoRA"><span class="toc-text">AdaLoRA</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By sagonimikakokomi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">welcome</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async defer src='/js/diy.js'></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div></body></html>