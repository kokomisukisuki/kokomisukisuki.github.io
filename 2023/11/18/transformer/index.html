<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>transformer | Hello</title><meta name="author" content="sagonimikakokomi"><meta name="copyright" content="sagonimikakokomi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="transformer主要内容转自The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time与Transformer学习笔记一：Positional Encoding（位置编码）) 简易模型首先，让我们将模型视为一个黑匣子。在机器翻译应用程序中，它将采用一种语">
<meta property="og:type" content="article">
<meta property="og:title" content="transformer">
<meta property="og:url" content="https://kokomisukisuki.github.io/2023/11/18/transformer/index.html">
<meta property="og:site_name" content="Hello">
<meta property="og:description" content="transformer主要内容转自The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time与Transformer学习笔记一：Positional Encoding（位置编码）) 简易模型首先，让我们将模型视为一个黑匣子。在机器翻译应用程序中，它将采用一种语">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.helloimg.com/images/2023/10/03/oHX5TE.jpg">
<meta property="article:published_time" content="2023-11-17T16:00:00.000Z">
<meta property="article:modified_time" content="2023-11-18T04:51:39.795Z">
<meta property="article:author" content="sagonimikakokomi">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.helloimg.com/images/2023/10/03/oHX5TE.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://kokomisukisuki.github.io/2023/11/18/transformer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"UNK1PFAIM3","apiKey":"82e2706c98842df0554f4f4c562132c7","indexName":"kokomisuki","hits":{"per_page":20},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: sagonimikakokomi","link":"链接: ","source":"来源: Hello","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'transformer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-18 12:51:39'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/color.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="/css/diy.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.css"/><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://ooo.0x0.ooo/2023/10/02/OnIWmL.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">213</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://www.helloimg.com/images/2023/10/03/oHxuAX.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Hello"><span class="site-name">Hello</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-11-17T16:00:00.000Z" title="发表于 2023-11-18 00:00:00">2023-11-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-11-18T04:51:39.795Z" title="更新于 2023-11-18 12:51:39">2023-11-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer"/>




<h1 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1><p>主要内容转自<a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time</a>与<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/454482273">Transformer学习笔记一：Positional Encoding（位置编码）)</a></p>
<h2 id="简易模型"><a href="#简易模型" class="headerlink" title="简易模型"></a>简易模型</h2><p>首先，让我们将模型视为一个黑匣子。在机器翻译应用程序中，它将采用一种语言的句子，然后输出另一种语言的翻译。</p>
<p><img src="http://jalammar.github.io/images/t/the_transformer_3.png" alt="img"></p>
<p>在黑匣子当中则是：</p>
<p><img src="http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png" alt="img"></p>
<p>编码组件是一堆编码器（encoder）（论文把其中6个编码器叠在一起——数字6没有什么神奇之处，人们肯定可以尝试其他排列）。解码（decoder）器是具有相同数量的解码器的堆栈。</p>
<p>编码器在结构上都是相同的（但它们不共享重量）。每个子层都分为<strong>两个子层</strong>：</p>
<p><img src="http://jalammar.github.io/images/t/Transformer_encoder.png" alt="img"></p>
<p>编码器的输入首先流经一个<strong>自注意层（self-attention）</strong>，该层可帮助编码器在对特定单词进行编码时查看输入句子中的其他单词。我们将在帖子的后面仔细研究自注意。</p>
<p>自注意层的输出被馈送到前馈神经网络（feed-forward neural network）。完全相同的前馈网络独立应用于每个位置。解码器具有这两层，<strong>但它们之间有一个注意力层</strong>（attention），可帮助解码器专注于输入句子的相关部分。</p>
<p><img src="http://jalammar.github.io/images/t/Transformer_decoder.png" alt="img"></p>
<h2 id="原理张量"><a href="#原理张量" class="headerlink" title="原理张量"></a>原理张量</h2><p>现在我们已经了解了模型的主要组件，让我们开始看看各种向量/张量，以及它们如何在这些组件之间流动，以将训练模型的输入转换为输出。</p>
<p>与一般的 NLP 应用程序一样，我们首先使用 <a target="_blank" rel="noopener" href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">embedding algorithm</a>将每个输入词转换为向量。</p>
<p><img src="http://jalammar.github.io/images/t/embeddings.png" alt="img"><br><em>每个单词都嵌入到大小为 512 的向量中。我们将用这些简单的框来表示这些向量。</em></p>
<p>嵌入仅发生在最底部的编码器中。所有编码器共有的是，它们接收一个向量列表，每个向量的大小为 512 – 在底部编码器中，这将是单词嵌入，但在其他编码器中，它将是编码器的输出，直接在下方。这个列表的大小是我们可以设置的超参数——基本上它是我们训练数据集中最长句子的长度。</p>
<p>将单词嵌入到输入序列中后，每个单词都会流经编码器的两层中的每一层。</p>
<p><img src="http://jalammar.github.io/images/t/encoder_with_tensors.png" alt="img"></p>
<p>在这里，我们开始看到 Transformer 的一个关键属性，即每个位置的单词在编码器中流经自己的路径。在自注意层中，这些路径之间存在依赖关系。但是，前馈层没有这些依赖关系，因此在流经前馈层时可以并行执行各种路径。</p>
<p>接下来，我们将示例切换为一个较短的句子，我们将查看编码器的每个子层中发生的情况。</p>
<h2 id="进入编码"><a href="#进入编码" class="headerlink" title="进入编码"></a>进入编码</h2><p>正如我们已经提到的，编码器接收一个向量列表作为输入。它通过将这些向量传递到“自注意”层，然后传递到前馈神经网络，然后将输出向上发送到下一个编码器来处理此列表。</p>
<p><img src="http://jalammar.github.io/images/t/encoder_with_tensors_2.png" alt="img"><br><em>每个位置的单词都经过一个自注意的过程。然后，它们各自通过一个前馈神经网络 - 完全相同的网络，每个向量分别流过它。</em></p>
<h2 id="高水平自注意"><a href="#高水平自注意" class="headerlink" title="高水平自注意"></a>高水平自注意</h2><p>假设以下句子是我们要翻译的输入句：</p>
<p><code>The animal didn&#39;t cross the street because it was too tired</code></p>
<p>这句话中的“it”指的是什么？它指的是街道还是动物？这对人类来说是一个简单的问题，但对算法来说却不那么简单。</p>
<p>当模型处理单词“它”时，自注意允许它将“它”与“动物”联系起来。</p>
<p>当模型处理每个单词（输入序列中的每个位置）时，自注意允许它查看输入序列中的其他位置，以寻找有助于为该单词提供更好编码的线索。</p>
<p>如果您熟悉 RNN，请考虑如何保持隐藏状态允许 RNN 将其先前处理的单词/向量的表示与当前正在处理的单词/向量的表示合并。自注意是 Transformer 用来将对其他相关单词的“理解”烘焙到我们当前正在处理的单词中的方法。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_self-attention_visualization.png" alt="img"><br><em>当我们在编码器 #5（堆栈中的顶部编码器）中对单词“it”进行编码时，部分注意力机制集中在“The Animal”上，并将其部分表示烘焙到“it”的编码中。</em></p>
<h2 id="自注意的细节"><a href="#自注意的细节" class="headerlink" title="自注意的细节"></a>自注意的细节</h2><p>让我们首先看看如何使用向量计算自注意，然后继续看看它实际上是如何实现的——使用矩阵。</p>
<p>计算自注意<strong>的第一步</strong>是从编码器的每个输入向量（在本例中为每个单词的嵌入）创建三个向量。因此，对于每个单词，我们创建一个查询向量（query简称Q）、一个键向量（key简称K）和一个值向量（value简称V）。<strong>这些向量是通过将嵌入乘以我们在训练过程中训练的三个矩阵来创建的。</strong></p>
<p>请注意，<strong>这些新向量的维度比嵌入向量小</strong>。它们的维数为 64，而嵌入和编码器输入/输出向量的维数为 512。它们不必更小，这是一种架构选择，可以使多注意（multiheaded attention）的计算（大部分）恒定。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_self_attention_vectors.png" alt="img"><br>将 x1 乘以 WQ 权重矩阵可得到 q1，即与该单词关联的“查询”向量。我们最终为输入句子中的每个单词创建“查询”、“键”和“值”投影。</p>
<p>什么是“查询”、“键”和“值”向量？</p>
<p>它们是用于计算和思考注意力的抽象概念。一旦你继续阅读下面如何计算注意力，你就会知道你需要知道的关于这些向量中的每一个所扮演的角色。</p>
<p>计算自注意<strong>的第二步</strong>是计算分数。假设我们正在计算这个例子中第一个词“思考”的自注意。我们需要根据这个单词对输入句子的每个单词进行评分。分数决定了当我们在某个位置对单词进行编码时，对输入句子的其他部分的关注程度。</p>
<p>通过取查询向量与我们正在评分的各个单词的关键向量的点积来计算分数。因此，如果我们处理位置 #1 中单词的自注意，第一个分数将是 q1 和 k1 的点积。第二个分数是 q1 和 k2 的点积。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_self_attention_score.png" alt="img"></p>
<p><strong>第三步和第四步</strong>是将分数除以 8（论文中使用的关键向量维度的平方根 –64。这导致梯度更稳定。此处可能有其他可能的值，但这是默认值），然后通过 softmax 操作传递结果。Softmax 对分数进行归一化，使它们都是正数，加起来等于 1。</p>
<p><img src="http://jalammar.github.io/images/t/self-attention_softmax.png" alt="img"></p>
<p>这个 softmax 分数决定了每个单词在此位置的表达量。显然，这个位置的单词将具有最高的softmax分数，但有时关注与当前单词相关的另一个单词是有用的。</p>
<p><strong>第五步</strong>是将每个值向量乘以 softmax 分数（准备对它们求和）。这里的直觉是保持我们想要关注的单词的值不变，并淹没不相关的单词（例如，将它们乘以 0.001 等微小数字）。</p>
<p><strong>第六步</strong>是求和加权值向量。这在这个位置（对于第一个单词）产生自注意层的输出。</p>
<p><img src="http://jalammar.github.io/images/t/self-attention-output.png" alt="img"></p>
<p>自注意计算到此结束。生成的向量是我们可以发送到前馈神经网络的向量。然而，在实际实现中，这种计算是以矩阵形式完成的，以便更快地处理。因此，现在我们已经看到了单词级别计算的直觉，让我们看看这一点。</p>
<h2 id="自注意矩阵计算"><a href="#自注意矩阵计算" class="headerlink" title="自注意矩阵计算"></a>自注意矩阵计算</h2><p><strong>第一步</strong>是计算 Query、Key 和 Value 矩阵。为此，我们将嵌入打包到矩阵 X 中，并将其乘以我们训练的权重矩阵（WQ、WK、WV）。</p>
<p><img src="http://jalammar.github.io/images/t/self-attention-matrix-calculation.png" alt="img"><br><em>X 矩阵中的每一行都对应于输入句子中的一个单词。我们再次看到嵌入向量（图中的 512 个或 4 个框）和 q/k/v 向量（图中的 64 个或 3 个框）的大小差异</em></p>
<p><strong>最后</strong>，由于我们处理的是矩阵，我们可以将步骤二到六浓缩在一个公式中，以计算自注意层的输出。</p>
<p><img src="http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" alt="img"></p>
<h2 id="长着许多头的野兽"><a href="#长着许多头的野兽" class="headerlink" title="长着许多头的野兽"></a>长着许多头的野兽</h2><p>该论文通过添加一种称为“多头”注意力的机制进一步完善了自注意层。这可以通过两种方式提高注意力层的性能：</p>
<ol>
<li>它扩展了模型聚焦于不同位置的能力。是的，在上面的例子中，z1 包含了一点点其他编码，但它可能由实际的单词本身主导。如果我们翻译一个句子，比如“动物没有过马路，因为它太累了”，知道“它”指的是哪个词会很有用。</li>
<li>它为注意力层提供了多个“表示子空间”。正如我们接下来将看到的，对于多头注意力，我们不仅有一组，而且有多组查询/键/值权重矩阵（Transformer 使用 8 个注意力头，因此我们最终为每个编码器/解码器提供 8 组注意力）。这些集合中的每一个都是随机初始化的。然后，在训练后，每个集合用于将输入嵌入（或来自较低编码器/解码器的向量）投影到不同的表示子空间中。</li>
</ol>
<p><img src="http://jalammar.github.io/images/t/transformer_attention_heads_qkv.png" alt="img"><br><em>通过多头注意力，我们为每个头维护单独的 Q/K/V 权重矩阵，从而产生不同的 Q/K/V 矩阵。和之前一样，我们将 X 乘以 WQ/WK/WV 矩阵以生成 Q/K/V 矩阵。</em></p>
<p>如果我们进行与上面概述的相同的自注意计算，只需使用不同的权重矩阵进行八次不同的计算，我们最终会得到八个不同的 Z 矩阵</p>
<p><img src="http://jalammar.github.io/images/t/transformer_attention_heads_z.png" alt="img"></p>
<p>这给我们带来了一些挑战。前馈层不需要八个矩阵，而是需要一个矩阵（每个单词一个向量）。因此，我们需要一种方法将这八个浓缩成一个矩阵。</p>
<p>我们是怎么做到的？我们将矩阵连接起来，然后将它们乘以附加的权重矩阵 WO。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png" alt="img"></p>
<p>这几乎就是多头自注意的全部内容。我意识到，这是相当多的矩阵。让我试着把它们都放在一个视觉对象中，这样我们就可以在一个地方看到它们</p>
<p><img src="http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="img"></p>
<p>现在我们已经触及了注意力头，让我们回顾一下之前的例子，看看当我们在例句中编码单词“it”时，不同的注意力头聚焦在哪里：</p>
<p><img src="http://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png" alt="img"><br>当我们对“它”这个词进行编码时，一个注意力集中在“动物”上，而另一个注意力则集中在“累了”上——从某种意义上说，模型对“it”这个词的表示与“动物”和“累了的”的一些表示差不多。</p>
<p>然而，如果我们把所有的注意力头都加到图片上，事情可能会更难解释：</p>
<p><img src="http://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png" alt="img"></p>
<h2 id="使用位置编码表示序列的顺序"><a href="#使用位置编码表示序列的顺序" class="headerlink" title="使用位置编码表示序列的顺序"></a>使用位置编码表示序列的顺序</h2><p>正如我们到目前为止所描述的那样，模型中缺少的一件事是解释输入序列中单词顺序的方法。</p>
<p>为了解决这个问题，转换器在每个输入嵌入中添加一个向量。这些向量遵循模型学习的特定模式，这有助于它确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将这些值添加到嵌入中，一旦嵌入向量被投影到 Q/K/V 向量中，并且在点积注意力期间，它们之间就会提供有意义的距离。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png" alt="img"><br>为了让模型了解单词的顺序，我们添加了位置编码向量——其值遵循特定的模式。</p>
<p>如果我们假设嵌入的维数为 4，则实际的位置编码将如下所示：</p>
<p><img src="http://jalammar.github.io/images/t/transformer_positional_encoding_example.png" alt="img"></p>
<p>这种模式可能是什么样子的？</p>
<p>在下图中，每一行对应一个向量的位置编码。因此，第一行是我们添加到输入序列中第一个单词嵌入的向量。每行包含 512 个值，每个值的值介于 1 和 -1 之间。我们对它们进行了颜色编码，因此图案是可见的。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png" alt="img"><br><em>嵌入大小为 512（列）的 20 个单词（行）的位置编码的真实示例。你可以看到它从中心分成两半。这是因为左半部分的值是由一个函数（使用正弦）生成的，而右半部分的值是由另一个函数（使用余弦）生成的。然后将它们连接起来形成每个位置编码向量。</em></p>
<p>本文（第 3.5 节）描述了位置编码的公式。您可以在 <a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py"><code>get_timing_signal_1d（）</code></a> 中看到用于生成位置编码的代码。这不是唯一可能的位置编码方法。然而，它的优势在于能够扩展到看不见的序列长度（例如，如果我们的训练模型被要求翻译一个句子比我们训练集中的任何一个句子都长）。</p>
<p><strong>2020年7月更新：</strong>上面显示的位置编码来自 Transformer 的 Tensor2Tensor 实现。论文中展示的方法略有不同，因为它不直接连接，而是将两个信号交织在一起。下图显示了它的外观。<a target="_blank" rel="noopener" href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb">下面是生成它的代码</a>：</p>
<p><img src="http://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png" alt=""></p>
<h2 id="残差"><a href="#残差" class="headerlink" title="残差"></a>残差</h2><p>在继续之前，我们需要在编码器架构中提到一个细节，即每个编码器中的每个子层（自注意，ffnn）都有一个残余连接，然后是<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">层归一化</a>步骤。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_resideual_layer_norm.png" alt="img"></p>
<p>如果我们要可视化与自注意相关的向量和层范数操作，它看起来像这样：</p>
<p><img src="http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" alt="img"></p>
<p>这也适用于解码器的子层。如果我们考虑一个由 2 个堆叠编码器和解码器组成的 Transformer，它看起来像这样：</p>
<p><img src="http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" alt="img"></p>
<h2 id="解码器端"><a href="#解码器端" class="headerlink" title="解码器端"></a>解码器端</h2><p>现在我们已经介绍了编码器方面的大多数概念，我们基本上也知道了解码器组件的工作原理。但是，让我们来看看它们是如何协同工作的。</p>
<p>编码器首先处理输入序列。然后，顶部编码器的输出被转换为一组注意力向量 K 和 V。每个解码器在其“编码器-解码器注意”层中使用这些，这有助于解码器专注于输入序列中的适当位置：</p>
<p><img src="http://jalammar.github.io/images/t/transformer_decoding_1.gif" alt="img"><br>完成编码阶段后，我们开始解码阶段。解码阶段的每个步骤都会从输出序列中输出一个元素（在本例中为英文翻译句子）。</p>
<p>以下步骤重复该过程，直到到达一个特殊符号，指示转换器解码器已完成其输出。每个步骤的输出在下一个时间步中被馈送到底部解码器，解码器像编码器一样冒出它们的解码结果。就像我们对编码器输入所做的那样，我们在这些解码器输入中嵌入并添加位置编码，以指示每个单词的位置。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="img"></p>
<p>解码器中的自注意层的运行方式与编码器中的自注意层略有不同：</p>
<p>在解码器中，只允许自注意层处理输出序列中较早的位置。这是通过在自注意计算中的softmax步骤之前屏蔽未来位置（将它们设置为-inf）来完成的。</p>
<p>“编码器-解码器注意力”层的工作方式与多头自注意类似，不同之处在于它从其下方的层创建其查询矩阵，并从编码器堆栈的输出中获取键和值矩阵。</p>
<h2 id="最终的线性层和-Softmax-层"><a href="#最终的线性层和-Softmax-层" class="headerlink" title="最终的线性层和 Softmax 层"></a>最终的线性层和 Softmax 层</h2><p>解码器堆栈输出浮点数向量。我们如何把它变成一个词？这是最后一个线性层的工作，然后是 Softmax 层。</p>
<p>线性层是一个简单的全连接神经网络，它将解码器堆栈产生的向量投射到一个更大的向量中，称为对数向量。</p>
<p>假设我们的模型知道 10,000 个独特的英语单词（我们模型的“输出词汇表”），这些单词是从其训练数据集中学习到的。这将使 logits 向量有 10,000 个单元格宽——每个单元格对应一个唯一单词的分数。这就是我们如何解释模型的输出，然后是线性层。</p>
<p>然后，softmax 层将这些分数转换为概率（全部为正数，加起来为 1.0）。选择概率最高的单元格，并生成与其关联的单词作为此时间步的输出。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_decoder_output_softmax.png" alt="img"><br>该图从底部开始，将向量作为解码器堆栈的输出生成。然后将其转换为输出词。</p>
<h2 id="训练回顾"><a href="#训练回顾" class="headerlink" title="训练回顾"></a>训练回顾</h2><p>现在我们已经通过一个经过训练的 Transformer 介绍了整个前向传递过程，那么看一眼训练模型的直觉会很有用。</p>
<p>在训练期间，未经训练的模型将经历完全相同的前向传递。但是，由于我们在标记的训练数据集上训练它，因此我们可以将其输出与实际正确的输出进行比较。</p>
<p>为了形象化这一点，我们假设我们的输出词汇表只包含六个单词（“a”、“am”、“i”、“thanks”、“student”和“<eos>”（“句子结尾”的缩写））。</p>
<p><img src="http://jalammar.github.io/images/t/vocabulary.png" alt="img"><br>我们模型的输出词汇表是在我们开始训练之前的预处理阶段创建的。</p>
<p>一旦我们定义了输出词汇表，我们就可以使用相同宽度的向量来指示词汇表中的每个单词。这也称为独热编码。因此，例如，我们可以使用以下向量来表示单词“am”：</p>
<p><img src="http://jalammar.github.io/images/t/one-hot-vocabulary-example.png" alt="img"><br>示例：输出词汇表的独热编码</p>
<p>在此回顾之后，让我们讨论模型的损失函数——我们在训练阶段优化的指标，以产生一个经过训练且希望非常准确的模型。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>假设我们正在训练我们的模型。假设这是我们在训练阶段的第一步，我们正在用一个简单的例子来训练它——将“merci（法语的感谢）”翻译成“thanks”。</p>
<p>这意味着，我们希望输出是一个概率分布，指示“thanks”这个词。但由于这个模型还没有被训练，这还不太可能发生。</p>
<p><img src="http://jalammar.github.io/images/t/transformer_logits_output_and_label.png" alt="img"><br>由于模型的参数（权重）都是随机初始化的，因此（未经训练的）模型会为每个单元格/单词生成具有任意值的概率分布。我们可以将其与实际输出进行比较，然后使用反向传播调整模型的所有权重，使输出更接近所需的输出。</p>
<p>如何比较两种概率分布？我们只是从另一个中减去一个。有关更多详细信息，请查看<a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-09-Visual-Information/">交叉熵</a>和 <a target="_blank" rel="noopener" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback-Leibler 散度</a>。</p>
<p>但请注意，这是一个过于简化的示例。更现实地说，我们将使用一个比一个单词更长的句子。例如，输入：“je suis étudiant”和预期输出：“i am a student”。这真正意味着，我们希望我们的模型连续输出概率分布，其中：</p>
<ul>
<li>每个概率分布都由宽度为 vocab_size 的向量表示（在我们的玩具示例中为 6，但更实际地说是 30,000 或 50,000 这样的数字）</li>
<li>第一个概率分布在与单词“i”关联的单元格中具有最高概率</li>
<li>第二个概率分布在与单词“am”关联的单元格中具有最高概率</li>
<li>依此类推，直到第五个输出分布指示 ‘’ 符号，该符号也有一个来自 10,000 个元素词汇表的单元格与之关联。<code>&lt;end of sentence&gt;</code></li>
</ul>
<p><img src="http://jalammar.github.io/images/t/output_target_probability_distributions.png" alt="img"><br>我们将针对一个示例句子的训练示例中训练模型的目标概率分布。</p>
<p>在足够大的数据集上训练模型足够长的时间后，我们希望生成的概率分布如下所示：</p>
<p><img src="http://jalammar.github.io/images/t/output_trained_model_probability_distributions.png" alt="img"><br>希望在训练时，该模型将输出我们期望的正确翻译。当然，这并不是真正表明这句话是否是训练数据集的一部分（参见：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=TIgfjmp-4BA">交叉验证</a>）。请注意，每个位置都会获得一点概率，即使它不太可能是该时间步的输出——这是 softmax 的一个非常有用的属性，有助于训练过程。</p>
<p>现在，由于模型一次生成一个输出，我们可以假设模型从该概率分布中选择概率最高的单词，并丢弃其余的单词。这是一种方法（称为贪婪解码）。另一种方法是保留前两个单词（例如，“I”和“a”），然后在下一步中运行模型两次：一次假设第一个输出位置是单词“I”，另一个假设第一个输出位置是单词“a”，并且考虑到位置 #1 和 #2 产生的误差较小，将保留。我们对位置 #2 和 #3 重复此操作……等。这种方法称为“光束搜索”，在我们的示例中，beam_size是两个（这意味着在任何时候，两个部分假设（未完成的翻译）都保存在内存中），top_beams也是两个（意味着我们将返回两个翻译）。这些都是可以试验的超参数。</p>
<h1 id="详细解读"><a href="#详细解读" class="headerlink" title="详细解读"></a>详细解读</h1><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><h2 id="一、为什么使用位置编码"><a href="#一、为什么使用位置编码" class="headerlink" title="一、为什么使用位置编码"></a>一、为什么使用位置编码</h2><p>在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足：</p>
<p>$input=input_embedding+positional_encoding$</p>
<p>这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512）</p>
<p>那么，我们为什么需要position encoding呢？在transformer的self-attention模块中，序列的输入输出如下（不了解self-attention没关系，这里只要关注它的输入输出就行）：</p>
<p><img src="https://pic2.zhimg.com/80/v2-095c25f4dc02c53d57a6edb1951694e5_720w.webp" alt=""></p>
<p>在self-attention模型中，输入是一整排的tokens，对于人来说，我们很容易知道tokens的位置信息，比如：<br>（1）绝对位置信息。a1是第一个token，a2是第二个token……<br>（2）相对位置信息。a2在a1的后面一位，a4在a2的后面两位……<br>（3）不同位置间的距离。a1和a3差两个位置，a1和a4差三个位置….<br>但是这些对于self-attention来说，是无法分辩的信息，因为self-attention的运算是无向的。因为，我们要想办法，把tokens的位置信息，喂给模型。</p>
<h2 id="二、构造位置编码的方法-演变历程"><a href="#二、构造位置编码的方法-演变历程" class="headerlink" title="二、构造位置编码的方法 /演变历程"></a>二、构造位置编码的方法 /演变历程</h2><h3 id="2-1-用整型值标记位置"><a href="#2-1-用整型值标记位置" class="headerlink" title="2.1 用整型值标记位置"></a>2.1 用整型值标记位置</h3><p>一种自然而然的想法是，给第一个token标记1，给第二个token标记2…，以此类推。<br>这种方法产生了以下几个主要问题：<br>（1）模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。<br>（2）模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。</p>
<h3 id="2-2-用-0-1-范围标记位置"><a href="#2-2-用-0-1-范围标记位置" class="headerlink" title="2.2 用[0,1]范围标记位置"></a>2.2 用[0,1]范围标记位置</h3><p>为了解决整型值带来的问题，可以考虑将位置值的范围限制在[0, 1]之内，其中，0表示第一个token，1表示最后一个token。比如有3个token，那么位置信息就表示成[0, 0.5, 1]；若有四个token，位置信息就表示成[0, 0.33, 0.69, 1]。<br>但这样产生的问题是，当序列长度不同时，token间的相对距离是不一样的。例如在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。</p>
<p>因此，我们需要这样一种位置表示方式，满足于：<br><strong>（1）它能用来表示一个token在序列中的绝对位置</strong><br><strong>（2）在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致</strong><br><strong>（3）可以用来表示模型在训练过程中从来没有看到过的句子长度。</strong></p>
<h3 id="2-3-用二进制向量标记位置"><a href="#2-3-用二进制向量标记位置" class="headerlink" title="2.3 用二进制向量标记位置"></a>2.3 用二进制向量标记位置</h3><p>考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。这时我们就很容易想到二进制编码。如下图，假设d_model = 3，那么我们的位置向量可以表示成：</p>
<p><img src="https://pic2.zhimg.com/80/v2-60d7a554b442eebe967d8e07eb941039_720w.webp" alt="img"></p>
<p>这下所有的值都是有界的（位于0，1之间），且transformer中的d_model本来就足够大，基本可以把我们要的每一个位置都编码出来了。</p>
<p>但是这种编码方式也存在问题：这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。假设d_model = 2，我们有4个位置需要编码，这四个位置向量可以表示成[0,0],[0,1],[1,0],[1,1]。我们把它的位置向量空间做出来：</p>
<p><img src="https://pic3.zhimg.com/80/v2-fd65171f6f594aa62cf14d64ce8d043e_720w.webp" alt="img"></p>
<p>如果我们能把离散空间（黑色的线）转换到连续空间（蓝色的线），那么我们就能解决位置距离不连续的问题。同时，我们不仅能用位置向量表示整型，我们还可以用位置向量来表示浮点型。</p>
<h3 id="2-4-用周期函数（sin）来表示位置"><a href="#2-4-用周期函数（sin）来表示位置" class="headerlink" title="2.4 用周期函数（sin）来表示位置"></a>2.4 用周期函数（sin）来表示位置</h3><p>回想一下，现在我们需要一个有界又连续的函数，最简单的，正弦函数sin就可以满足这一点。我们可以考虑把位置向量当中的每一个元素都用一个sin函数来表示，则第t个token的位置向量可以表示为：</p>
<p>$PE_t=[sin(\frac{1}{2^0}t),sin(\frac{1}{2^1}t)\ldots,sin(\frac{1}{2^{i-1}}t),\ldots,sin(\frac{1}{2^{d_{model}-1}}t)]$</p>
<p>结合下图，来理解一下这样设计的含义。图中每一行表示一个$PE_t$，每一列表示 $PE_t$ 中的第i个元素。旋钮用于调整精度，越往右边的旋钮，需要调整的精度越大，因此指针移动的步伐越小。每一排的旋钮都在上一排的基础上进行调整（函数中t的作用）。通过频率 $\frac1{2^{i-1}}$ 来控制sin函数的波长，频率不断减小，则波长不断变大，此时sin函数对t的变动越不敏感，以此来达到越向右的旋钮，指针移动步伐越小的目的。 这也类似于二进制编码，每一位上都是0和1的交互，越往低位走（越往左边走），交互的频率越慢。</p>
<p><img src="https://pic1.zhimg.com/80/v2-eb3d818037adb892dafcf26a6ef433cc_720w.webp" alt="img"></p>
<p><img src="https://pic1.zhimg.com/80/v2-5887aa90c56ff14b494bb670adc5bea4_720w.webp" alt="img"></p>
<p><em>越往左边走，交互频率越慢</em></p>
<p>由于sin是周期函数，因此从纵向来看，如果函数的频率偏大，引起波长偏短，则不同t下的位置向量可能出现重合的情况。比如在下图中(d_model = 3），图中的点表示每个token的位置向量，颜色越深，token的位置越往后，在频率偏大的情况下，位置向量点连成了一个闭环，靠前位置（黄色）和靠后位置（棕黑色）竟然靠得非常近：</p>
<p><img src="https://pic3.zhimg.com/80/v2-a76698f2225c5ed7bc49b2259548cf2a_720w.webp" alt="img"></p>
<p>为了避免这种情况，我们尽量将函数的波长拉长。一种简单的解决办法是同一把所有的频率都设成一个非常小的值。因此在transformer的论文中，采用了 $\frac1{10000^{i/(d}model^{-1)}}$ 这个频率（这里i其实不是表示第i个位置，但是大致意思差不多，下面会细说）</p>
<p>总结一下，到这里我们把位置向量表示为：</p>
<p>$PE_t=[sin(w_0t),sin(w_1t)\ldots,sin(w_{i-1}t),\ldots,sin(w_{d_{model}-1}t)]$</p>
<h3 id="2-5-用sin和cos交替来表示位置"><a href="#2-5-用sin和cos交替来表示位置" class="headerlink" title="2.5 用sin和cos交替来表示位置"></a>2.5 用sin和cos交替来表示位置</h3><p>目前为止，我们的位置向量实现了如下功能：<br>（1）每个token的向量唯一（每个sin函数的频率足够小）<br>（2）位置向量的值是有界的，且位于连续空间中。模型在处理位置向量时更容易泛化，即更好处理长度和训练数据分布不一致的序列（sin函数本身的性质）</p>
<p>那现在我们对位置向量再提出一个要求，<strong>不同的位置向量是可以通过线性转换得到的</strong>。这样，我们不仅能表示一个token的绝对位置，还可以表示一个token的相对位置，即我们想要：</p>
<p>$PE_{t+\triangle t}=T_{\triangle t}*PE_t$</p>
<p>这里，T表示一个线性变换矩阵。观察这个目标式子，联想到在向量空间中一种常用的线形变换——旋转。在这里，我们将t想象为一个角度，那么 △t就是其旋转的角度，则上面的式子可以进一步写成：</p>
<p>$\begin{pmatrix}\sin(t+\bigtriangleup t)\\\cos((t+\bigtriangleup t)\end{pmatrix}=\begin{pmatrix}\cos\bigtriangleup t&amp;\sin\bigtriangleup t\-\sin\bigtriangleup t&amp;\cos\bigtriangleup t\end{pmatrix}\begin{pmatrix}\sin t\\\cos t\end{pmatrix}$</p>
<p>有了这个构想，我们就可以把原来元素全都是sin函数的 $PE_t$ 做一个替换，我们让位置两两一组，分别用sin和cos的函数对来表示它们，则现在我们有：</p>
<p>$\begin{gathered}PE_t=[sin(w_0t),cos(w_0t),sin(w_1t),cos(w_1t),\ldots,sin(w_{\frac{d_{model}}{2}-1}t),\\cos(w_{\frac{d_{model}}{2}-1}t)]\end{gathered}$</p>
<p>于是，可以用线性变换将$PE_{t}\text{ 转变为 }PE_{t+\triangle t}$</p>
<h2 id="三、Transformer中位置编码方法：Sinusoidal-functions"><a href="#三、Transformer中位置编码方法：Sinusoidal-functions" class="headerlink" title="三、Transformer中位置编码方法：Sinusoidal functions"></a>三、Transformer中位置编码方法：Sinusoidal functions</h2><h3 id="3-1-Transformer-位置编码定义"><a href="#3-1-Transformer-位置编码定义" class="headerlink" title="3.1 Transformer 位置编码定义"></a>3.1 Transformer 位置编码定义</h3><p>有了上面的演变过程后，现在我们就可以正式来看transformer中的位置编码方法了。</p>
<p>$\begin{aligned}<br>&amp;\text{定义:} \\&amp;\text{t是这个token的实际位置}\\&amp;PE_t\in\mathbb{R}^d\text{ 是这个token的位置向量, }PE_t^{(i)}\text{ 表示这个位置向量里的第i个元素} \\<br>&amp;d_{model}\text{ 是这个token的维度(在论文中,是512}) \\<br>&amp;\text{则 }PE_{t}^{(i)}\text{ 可以表示为:} \\<br>&amp; \left.PE_t^{(i)}=\left\{\begin{array}{lr}\sin(w_kt),&amp;ifi=2k\\\cos(w_kt),&amp;ifi=2k+1\end{array}\right.\right.  \\<br>&amp;\text{这里:} \\<br>&amp;w_{k}=\frac{1}{10000^{2k/d_{model}}} \\<br>&amp;i=0,1,2,3,\ldots,\frac{d_{model}}{2}-1<br>\end{aligned}$</p>
<p>$\begin{gathered}<br>\text{看得有点懵不要紧,这个意思和2.5中的意思是一模一样的,把512维的向量两两一组,每组都是} \\<br>\text{一个}\mathrm{sin}\text{和一个}\mathrm{cos}\text{,这两个函数共享同一个频率 }w_i\text{ ,一共有256组,由于我们从}0\text{开始编号,所}\\<br>\text{以最后一组编号是255。}\sin/\cos\text{函数的波长(由 }w_i\text{ 决定)则从 }2\pi\text{ 增长到 }2\pi*10000<br>\end{gathered}$</p>
<h3 id="3-2-Transformer位置编码可视化"><a href="#3-2-Transformer位置编码可视化" class="headerlink" title="3.2 Transformer位置编码可视化"></a>3.2 Transformer位置编码可视化</h3><p>下图是一串序列长度为50，位置编码维度为128的位置编码可视化结果：</p>
<p><img src="https://pic1.zhimg.com/80/v2-b6c64586260ebed24339052adec7bca8_720w.webp" alt=""></p>
<p>即为<img src="http://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png" alt=""></p>
<p>可以发现，由于sin/cos函数的性质，位置向量的每一个值都位于[-1, 1]之间。同时，纵向来看，图的右半边几乎都是蓝色的，这是因为越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。而越往左边走，颜色交替的频率越频繁。</p>
<h3 id="3-3-Transformer位置编码的重要性质"><a href="#3-3-Transformer位置编码的重要性质" class="headerlink" title="3.3 Transformer位置编码的重要性质"></a>3.3 Transformer位置编码的重要性质</h3><p>让我们再深入探究一下位置编码的性质。</p>
<p><strong>(1) 性质一：两个位置编码的点积(dot product)仅取决于偏移量</strong> △t <strong>，也即两个位置编码的点积可以反应出两个位置编码间的距离。</strong></p>
<p>$\begin{aligned}<br>PE_{t}^{T}*PE_{t+\triangle t}&amp; \begin{aligned}&amp;=\sum_{i=0}^{\frac{d_{model}}{2}{-1}}[sin(w_it)sin(w_i(t+\triangle t)+cos(w_it)cos(w_i(t+\triangle t)]\end{aligned}  \\<br>&amp;=\sum_{i=0}^{\frac{d_{model}}2-1}cos(w_i(t-(t+\bigtriangleup t))) \\<br>&amp;=\sum_{i=0}^{\frac{d_{model}}2-1}cos(w_i\bigtriangleup t)<br>\end{aligned}$</p>
<p><strong>(2) 性质二：位置编码的点积是无向的，即 $PE_{t}^{T}<em>PE_{t+\triangle t}=PE_{t}^{T}</em>PE_{t-\triangle t}$</strong></p>
<p>证明：<br>由于cos函数的对称性，基于性质1，这一点即可证明。<br>我们可以分别训练不同维度的位置向量，然后以某个位置向量 $PE_{t}$ 为基准，去计算其左右和它相距 △t 的位置向量的点积，可以得到如下结果：</p>
<p><img src="https://pic3.zhimg.com/80/v2-3c9fd774843c50cfceca7e47ffd18d3a_720w.webp" alt="img"></p>
<p>这里横轴的k指的就是 △t ，可以发现，距离是对成分布的，且总体来说， △t 越大或者越小的时候，内积也越小，可以反馈距离的远近。也就是说，虽然位置向量的点积可以用于表示<strong>距离(distance-aware)</strong>，但是它却不能用来表示位置的<strong>方向性(lack-of-directionality)</strong>。</p>
<p>当位置编码随着input被喂进attention层时，采用的映射方其实是：</p>
<p>$PE_t^TW_Q^TW_KPE_{t+k}$</p>
<p>$\begin{aligned}<br>&amp;\text{这里 }W_Q^T\text{ 和 }W_K\text{ 表示self-attention中的query和key参数矩阵,他们可以被简写成 }W\text{ 表示} \\<br>&amp;\text{attention score的矩阵,到这里看不懂也没事,在self-attention的笔记里会说明的)。我们可以} \\<br>&amp;\text{随机初始化两组 }W_1\text{,}W_2\text{,然后将 }PE_t^TW_1PE_{t+k}\text{,}PE_t^TW_2PE_{t+k}\text{ 和 }PE_t^TPE_{t+k}\text{ 这} \\<br>&amp;\text{三个内积进行比较,得到的结果如下:}<br>\end{aligned}$</p>
<p><img src="https://pic2.zhimg.com/80/v2-49c33f7f895a632fbdef7c4b2e2fe409_720w.webp" alt="img"></p>
<p>绿色和黄色即是 W1 和 W2 的结果。可以发现，进入attention层之后，内积的<strong>距离意识(distance-aware)</strong>的模式也遭到了破坏。<strong>更详细的细节，可以参见复旦大学这一篇用transformer做NER的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1911.04474.pdf">论文</a>中。</strong></p>
<p>在Transformer的论文中，比较了用positional encoding和learnable position embedding(让模型自己学位置参数）两种方法，得到的结论是两种方法对模型最终的衡量指标差别不大。不过在后面的BERT中，已经改成用learnable position embedding的方法了，也许是因为positional encoding在进attention层后一些优异性质消失的原因（猜想）。Positional encoding有一些想象+实验+论证的意味，而编码的方式也不只这一种，比如把sin和cos换个位置，依然可以用来编码。关于positional encoding，我也还在持续探索中。</p>
<h2 id="自注意"><a href="#自注意" class="headerlink" title="自注意"></a>自注意</h2><h2 id="一、基于结构"><a href="#一、基于结构" class="headerlink" title="一、基于结构"></a>一、基于结构</h2><h2 id="二、Attention构造"><a href="#二、Attention构造" class="headerlink" title="二、Attention构造"></a>二、Attention构造</h2><h3 id="2-1-Attention的基本运作方式"><a href="#2-1-Attention的基本运作方式" class="headerlink" title="2.1 Attention的基本运作方式"></a>2.1 Attention的基本运作方式</h3><p>首先，来看RNN这样一个用于处理序列数据的经典模型。</p>
<p><img src="https://pic4.zhimg.com/80/v2-0b316642170b60aed0ab21c638a5a07f_720w.webp" alt=""></p>
<p>在RNN当中，tokens是一个一个被喂给模型的。比如在a3的位置，模型要等a1和a2的信息都处理完成后，才可以生成a3。这样的作用机制，使得RNN存在以下几个问题：<br><strong>(1) Sequential operations的复杂度随着序列长度的增加而增加。</strong><br>这是指模型下一步计算的等待时间，在RNN中为O(N)。该复杂度越大，模型并行计算的能力越差，反之则反。<br><strong>(2) Maximum Path length的复杂度随着序列长度的增加而增加。</strong><br>这是指信息从一个数据点传送到另一个数据点所需要的距离，在RNN中同样为O(N)，距离越大，则在传送的过程中越容易出现信息缺失的情况，即数据点对于远距离处的信息，是很难“看见”的。</p>
<p>那么，在处理序列化数据的时候，是否有办法，在提升模型的并行运算能力的同时，对于序列中的每个token，也能让它不损失信息地看见序列里的其他tokens呢？</p>
<p>Attention就作为一种很好的改进办法出现了。</p>
<p><img src="https://pic2.zhimg.com/80/v2-095c25f4dc02c53d57a6edb1951694e5_720w.webp" alt=""></p>
<p>如图，蓝色方框为一个attention模型。在每个位置，例如在a2处产生b2时，attention将会同时看过a1到a4的每个token。此外，每个token生成其对应的输出的过程是同时进行的，计算不需要等待。下面来看attention内部具体的运算过程。</p>
<h3 id="2-2-Attention的计算过程图解"><a href="#2-2-Attention的计算过程图解" class="headerlink" title="2.2 Attention的计算过程图解"></a>2.2 Attention的计算过程图解</h3><p><strong>2.2.1 Self-attention</strong></p>
<p><strong>（1）计算框架</strong></p>
<p>Self-attention的意思是，我们给Attention的输入都来自同一个序列，其计算方式如下：</p>
<p><img src="https://pic2.zhimg.com/80/v2-980e1c0abb79f7d16785a59d55618991_720w.webp" alt=""></p>
<p>这张图所表示的大致运算过程是：<br>对于每个token，先产生三个向量query，key，value：</p>
<ul>
<li>query向量类比于询问。某个token问：“其余的token都和我有多大程度的相关呀？”</li>
<li>key向量类比于索引。某个token说：“我把每个询问内容的回答都压缩了下装在我的key里”</li>
<li>value向量类比于回答。某个token说：“我把我自身涵盖的信息又抽取了一层装在我的value里”</li>
</ul>
<p>以图中的token a2为例：</p>
<ul>
<li>它产生一个query，每个query都去和别的token的key做“<strong>某种方式</strong>”的计算，得到的结果我们称为attention score（即为图中的$\alpha $）。则一共得到四个attention score。（attention score又可以被称为attention weight）。</li>
<li>将这四个score分别乘上每个token的value，我们会得到四个抽取信息完毕的向量。</li>
<li>将这四个向量相加，就是最终a2过attention模型后所产生的结果b2。</li>
</ul>
<p><strong>（2）产生query，key和value</strong></p>
<p><strong>（3）计算attention score</strong></p>
<p>于是有：</p>
<p><img src="https://pic4.zhimg.com/80/v2-a5856289c63896fbb4dd9e7151bbde67_720w.webp" alt=""></p>
<p><strong>之所以进行scaling，是为了使得在softmax的过程中，梯度下降得更加稳定，避免因为梯度过小而造成模型参数更新的停滞</strong>。</p>
<p>（数学推导暂略）</p>
<h3 id="2-3-Masked-Attention"><a href="#2-3-Masked-Attention" class="headerlink" title="2.3 Masked Attention"></a>2.3 Masked Attention</h3><p>有时候，我们并不想在做attention的时候，让一个token看到整个序列，我们只想让它看见它左边的序列，而要把右边的序列遮蔽（Mask）起来。例如在transformer的decoder层中，我们就用到了masked attention，这样的操作可以理解为模型为了防止decoder在解码encoder层输出时“作弊”，提前看到了剩下的答案，因此需要强迫模型根据输入序列左边的结果进行attention。</p>
<p>首先，我们按照前文所说，正常算attention score，然后我们用一个MASK矩阵去处理它（这里的+号并不是表示相加，只是表示提供了位置覆盖的信息）。在MASK矩阵标1的地方，也就是需要遮蔽的地方，我们把原来的值替换为一个很小的值（比如-1e09），而在MASK矩阵标0的地方，我们保留原始的值。这样，在进softmax的时候，那些被替换的值由于太小，就可以自动忽略不计，从而起到遮蔽的效果。</p>
<p>举例来说明MASK矩阵的含义，每一行表示对应位置的token。例如在第一行第一个位置是0，其余位置是1，这表示第一个token在attention时，只看到它自己，它右边的tokens是看不到的。以此类推。</p>
<h3 id="2-4-Multihead-Attention"><a href="#2-4-Multihead-Attention" class="headerlink" title="2.4 Multihead Attention"></a>2.4 Multihead Attention</h3><p>在图像中，我们知道有不同的channel，每一个channel可以用来识别一种模式。如果我们对一张图采用attention，比如把这张图的像素格子拉平成一列，那么我们可以对每个像素格子训练不同的head，每个head就类比于一个channel，用于识别不同的模式。</p>
<p>而在NLP中，这种模式识别同样重要。比如第一个head用来识别词语间的指代关系(某个句子里有一个单词it，这个it具体指什么呢），第二个head用于识别词语间的时态关系（看见yesterday就要用过去式）等等。</p>
<p>图8展示了multihead attention的运作方式。设头的数量为num_heads，那么本质上，就是训练num_heads个 ，$W^Q，W^K,W^V $个矩阵，用于生成num_heads个 Q,K,V, 结果。每个结果的计算方式和单头的attention的计算方式一致。最终将生成的b连接起来生成最后的结果。图9详细展示了8个head的矩阵化的运算过程，由于拆分成了多头，则此时有<br>$k_dim=v_dim=d_model//num_heads$<br>也就是说， ，$W^Q，W^K,W^V $的维度变为 ($d_model,d_model//num_heads$)。按照这个规则拆分后，多头的运算量和原来单头的运算量一样。</p>
<p><img src="https://pic2.zhimg.com/80/v2-ecb69d3f93641e86d1a40bd0ee097531_720w.webp" alt=""></p>
<p>同时在下图中，在输出部分出现了一个 $W^O $矩阵，这个矩阵用于将拼接起来的多头输出转换为最终总输出</p>
<p><img src="https://pic3.zhimg.com/80/v2-19bfdd8617556d67a399f41710c3c796_720w.webp" alt=""></p>
<p>将每个head上的attention score分数打出，可以具象化地感受每个head的关注点，以入句子”The animal didn’t cross the streest because it was too tired”为例，可视化代码可<a href="https://link.zhihu.com/?target=https%3A//colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb%23scrollTo%3DOJKU36QAfqOC">点此</a>（存在Google colab上，需要翻墙）。</p>
<p><img src="https://pic4.zhimg.com/80/v2-8075d11834ea09b147ad390cc43cb4b3_720w.webp" alt=""></p>
<p>如上图，颜色越深表示attention score越大，我们构造并连接五层的attention模块，可以发现it和animal，street关系密切。现在我们把8个头全部加上去，见下图。</p>
<p><img src="https://pic4.zhimg.com/80/v2-5dd84c5a42c79c23ed03d7d646a636d3_720w.webp" alt=""></p>
<p>此时一种颜色表示一个头下attention score的分数，可以看出，不同的头所关注的点各不相同。</p>
<h2 id="三、Attention代码实践"><a href="#三、Attention代码实践" class="headerlink" title="三、Attention代码实践"></a>三、Attention代码实践</h2><p>这里提供一个Mutihead Attention的python实现方法，它可以快速帮助我们了解一个attention层的计算过程，同时可以很方便地打出中间步骤。Tensorflow和Pytorch的源码里有更为工业化的实现方式，包加速运算、引入bias，自定义维度等等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span>, <span class="type">Any</span>, <span class="type">Union</span>, <span class="type">Callable</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                num_heads: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                d_model: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                dropout: <span class="built_in">float</span>=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span>, <span class="string">&quot;d_model must be divisible by num_heads&quot;</span></span><br><span class="line">        <span class="comment"># Assume v_dim always equals k_dim</span></span><br><span class="line">        self.k_dim = d_model // num_heads</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.proj_weights = clones(nn.Linear(d_model, d_model), <span class="number">4</span>) <span class="comment"># W^Q, W^K, W^V, W^O</span></span><br><span class="line">        self.attention_score = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                query:Tensor, </span></span><br><span class="line"><span class="params">                key: Tensor, </span></span><br><span class="line"><span class="params">                value: Tensor, </span></span><br><span class="line"><span class="params">                mask:<span class="type">Optional</span>[Tensor]=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            query: shape (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">            key: shape (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">            value: shape (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">            mask: shape (batch_size, seq_len, seq_len). Since we assume all data use a same mask, so</span></span><br><span class="line"><span class="string">                  here the shape also equals to (1, seq_len, seq_len)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            out: shape (batch_size, seq_len, d_model). The output of a multihead attention layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Apply W^Q, W^K, W^V to generate new query, key, value</span></span><br><span class="line">        query, key, value \</span><br><span class="line">            = [proj_weight(x).view(batch_size, -<span class="number">1</span>, self.num_heads, self.k_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">for</span> proj_weight, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.proj_weights, [query, key, value])] <span class="comment"># -1 equals to seq_len</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Calculate attention score and the out</span></span><br><span class="line">        out, self.attention_score = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; output</span></span><br><span class="line">        out = out.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(batch_size, -<span class="number">1</span>, self.num_heads * self.k_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4) Apply W^O to get the final output</span></span><br><span class="line">        out = self.proj_weights[-<span class="number">1</span>](out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">        <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query: Tensor, </span></span><br><span class="line"><span class="params">              key: Tensor, </span></span><br><span class="line"><span class="params">              value: Tensor, </span></span><br><span class="line"><span class="params">              mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">              dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Define how to calculate attention score</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        query: shape (batch_size, num_heads, seq_len, k_dim)</span></span><br><span class="line"><span class="string">        key: shape(batch_size, num_heads, seq_len, k_dim)</span></span><br><span class="line"><span class="string">        value: shape(batch_size, num_heads, seq_len, v_dim)</span></span><br><span class="line"><span class="string">        mask: shape (batch_size, num_heads, seq_len, seq_len). Since our assumption, here the shape is</span></span><br><span class="line"><span class="string">              (1, 1, seq_len, seq_len)</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        out: shape (batch_size, v_dim). Output of an attention head.</span></span><br><span class="line"><span class="string">        attention_score: shape (seq_len, seq_len).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    k_dim = query.size(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># shape (seq_len ,seq_len)，row: token，col: that token&#x27;s attention score</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(k_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e10</span>)</span><br><span class="line"></span><br><span class="line">    attention_score = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        attention_score = dropout(attention_score)</span><br><span class="line">        </span><br><span class="line">    out = torch.matmul(attention_score, value)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> out, attention_score <span class="comment"># shape: (seq_len, v_dim), (seq_len, seq_lem)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    d_model = <span class="number">8</span></span><br><span class="line">    seq_len = <span class="number">3</span></span><br><span class="line">    batch_size = <span class="number">6</span></span><br><span class="line">    num_heads = <span class="number">2</span></span><br><span class="line">    <span class="comment"># mask = None</span></span><br><span class="line">    mask = torch.tril(torch.ones((seq_len, seq_len)), diagonal = <span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">input</span> = torch.rand(batch_size, seq_len, d_model)</span><br><span class="line">    multi_attn = MultiHeadedAttention(num_heads = num_heads, d_model = d_model, dropout = <span class="number">0.1</span>)</span><br><span class="line">    out = multi_attn(query = <span class="built_in">input</span>, key = <span class="built_in">input</span>, value = <span class="built_in">input</span>, mask = mask)</span><br><span class="line">    <span class="built_in">print</span>(out.shape)</span><br></pre></td></tr></table></figure>
<h2 id="批量以及层标准化"><a href="#批量以及层标准化" class="headerlink" title="批量以及层标准化"></a>批量以及层标准化</h2><h2 id="一、Batch-Normalization"><a href="#一、Batch-Normalization" class="headerlink" title="一、Batch Normalization"></a>一、Batch Normalization</h2><h3 id="1-1-提出背景"><a href="#1-1-提出背景" class="headerlink" title="1.1 提出背景"></a>1.1 提出背景</h3><p>Batch Normalization（以下简称BN）的方法最早由<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1502.03167.pdf">Ioffe&amp;Szegedy</a>在2015年提出，主要用于解决在深度学习中产生的ICS（Internal Covariate Shift）的问题。若模型输入层数据分布发生变化，则模型在这波变化数据上的表现将有所波动，输入层分布的变化称为Covariate Shift，解决它的办法就是常说的Domain Adaptation。同理，在深度学习中，第L+1层的输入，也可能随着第L层参数的变动，而引起分布的变动。这样每一层在训练时，都要去适应这样的分布变化，使得训练变得困难。这种层间输入分布变动的情况，就是Internal Covariate Shift。而BN提出的初衷就是为了解决这一问题。</p>
<p>$\begin{aligned}&amp;z^{[L]}=W^{[L]}*A^{[L-1]}+b^{[L]}(\text{线性变化层})\\&amp;\mathrm{A}^{[L]}=g^{[L]}(Z^{[L]})(\text{非线性变化}/\text{激活函数层})\end{aligned}$</p>
<p>$\begin{aligned}&amp;\text{(ICS:随着梯度下降的进行,}W^{[L]}\text{ 和 }b^{[L]}\text{ 都会被更新,则 }Z^{[L]}\text{ 的分布改变,进而影响 }A^{[L]}\\&amp;\text{分布,也就是第}L+1\text{层的输出})\end{aligned}$</p>
<h3 id="1-1-1-ICS所带来的问题"><a href="#1-1-1-ICS所带来的问题" class="headerlink" title="1.1.1 ICS所带来的问题"></a>1.1.1 ICS所带来的问题</h3><p><strong>（1）在过激活层的时候，容易陷入激活层的梯度饱和区，降低模型收敛速度。</strong><br>这一现象发生在我们对模型使用饱和激活函数(saturated activation function)，例如sigmoid，tanh时。如下图激活函数：</p>
<p><img src="https://pic2.zhimg.com/80/v2-0dcf1fbcd61ae858859cdc0b475ab875_720w.webp" alt=""></p>
<p>可以发现当绝对值越大时，数据落入图中两端的梯度饱和区（saturated regime），造成梯度消失，进而降低模型收敛速度。当数据分布变动非常大时，这样的情况是经常发生的。当然，解决这一问题的办法可以采用非饱和的激活函数，例如ReLu。<br><strong>（2）需要采用更低的学习速率，这样同样也降低了模型收敛速度。</strong><br>如前所说，由于输入变动大，上层网络需要不断调整去适应下层网络，因此这个时候的学习速率不宜设得过大，因为梯度下降的每一步都不是“确信”的。</p>
<h3 id="1-1-2-解决ICS的常规方法"><a href="#1-1-2-解决ICS的常规方法" class="headerlink" title="1.1.2 解决ICS的常规方法"></a>1.1.2 解决ICS的常规方法</h3><p>综合前面，在BN提出之前，有几种用于解决ICS的常规办法：</p>
<ul>
<li>采用非饱和激活函数</li>
<li>更小的学习速率</li>
<li>更细致的参数初始化办法</li>
<li>数据白化（whitening）</li>
</ul>
<p>其中，最后一种办法是在模型的每一层输入上，采用一种线性变化方式（例如PCA），以达到如下效果：</p>
<ul>
<li>使得输入的特征具有相同的均值和方差。例如采用PCA，就让所有特征的分布均值为0，方差为1</li>
<li>去除特征之间的相关性。</li>
</ul>
<p>然而在每一层使用白化，给模型增加了运算量。而小心地调整学习速率或其他参数，又陷入到了超参调整策略的复杂中。因此，BN作为一种更优雅的解决办法被提出了。</p>
<h3 id="1-2-BN的实践"><a href="#1-2-BN的实践" class="headerlink" title="1.2 BN的实践"></a>1.2 BN的实践</h3><h3 id="1-2-1-思路"><a href="#1-2-1-思路" class="headerlink" title="1.2.1 思路"></a>1.2.1 思路</h3><ul>
<li>对每一个batch进行操作，使得对于这一个batch中所有的输入数据，它们的每一个特征都是均值为0，方差为1的分布</li>
<li>单纯把所有的输入限制为(0,1)分布也是不合理的，这样会降低数据的表达能力（第L层辛苦学到的东西，这里都暴力变成（0,1）分布了）。因此需要再加一个线性变换操作，让数据恢复其表达能力。这个线性变化中的两个参数 $\gamma,\beta$ 是需要模型去学习的。</li>
</ul>
<p>整个BN的过程可以见下图：</p>
<p><img src="https://pic2.zhimg.com/80/v2-bd1fa944470997e03e89a64e9ce2b469_720w.webp" alt=""></p>
<p>上图所示的是2D数据下的BN，而在NLP或图像任务中，我们通常遇到3D或4D的数据，例如：</p>
<ul>
<li>图像中的数据维度：（N, C, H, W)。其中N表示数据量（图数），C表示channel数，H表示高度，W表示宽度。</li>
<li>NLP中的数据为度：（B, S, E）。其中B表示批量大小，S表示序列长度，F表示序列里每个token的embedding向量维度。</li>
</ul>
<p>如下图，它们在执行BN时，在图中每一个蓝色的平面上求取 $\mu, \sigma$ ，同时让模型自己学习 $\gamma,\beta$ 。其中”H,W”表示的是”H<em>W”，即每一个channel里pixel的数量。为了表达统一，这张图用作NLP任务说明时，可将(N, C, H</em>W)分别理解成(B, S, E)。</p>
<p><img src="https://pic4.zhimg.com/80/v2-6c8e8c02faae9531aab64f7c897cefc3_720w.webp" alt=""></p>
<h3 id="1-2-1-训练过程中的BN"><a href="#1-2-1-训练过程中的BN" class="headerlink" title="1.2.1 训练过程中的BN"></a>1.2.1 训练过程中的BN</h3><p>配合上面的图例，我们来具体写一下训练中BN的计算方式。<br>假设一个batch中有m个样本，则在神经网络的某一层中，我们记第i个样本在改层第j个神经元中，经过线性变换后的输出为$ Z^i_j$ ，则BN的过程可以写成（图中的每一个红色方框）：</p>
<p>$\begin{aligned}<br>&amp;\mu_{j} =\frac1m\sum_{i=1}^{m}Z_{j}^{i}  \\<br>&amp;\sigma_{j}^{2} =\frac1m\sum_{i=1}^m(Z_j^i-\mu_j)^2  \\<br>&amp;\tilde{Z}_{j} =\gamma_{j}\frac{Z_{j}-\mu_{j}}{\sqrt{\sigma_{j}^{2}+\epsilon}}+\beta_{j}<br>\end{aligned}$</p>
<h3 id="1-2-2-测试过程中的BN"><a href="#1-2-2-测试过程中的BN" class="headerlink" title="1.2.2 测试过程中的BN"></a>1.2.2 测试过程中的BN</h3><p>在训练过程里，我们等一个batch的数据都装满之后，再把数据装入模型，做BN。但是在测试过程中，我们却更希望模型能来一条数据就做一次预测，而不是等到装满一个batch再做预测。也就是说，我们希望测试过程共的数据不依赖batch，每一条数据都有一个唯一预测结果。这就产生了训练和测试中的gap：测试里的 $\mu,\sigma$ 要怎么算呢？一般来说有两种方法。</p>
<p><strong>（1）用训练集中的均值和方差做测试集中均值和方差的无偏估计</strong></p>
<p>保留训练模型中，<strong>每一组</strong>batch的<strong>每一个</strong>特征在<strong>每一层</strong>的 $\mu_{batch},\sigma^2_{batch}$，这样我们就可以得到测试数据均值和方差的无偏估计：</p>
<p>$\begin{aligned}<br>\mu_{test}&amp; =\mathbb{E}(\mu_{batch})  \\<br>\sigma_{test}^{2}&amp; =\frac m{m-1}\mathbb{E}(\sigma_{batch}^2)  \\<br>BN(X_{test})&amp; =\gamma\frac{X_{test}-\mu_{test}}{\sqrt{\sigma_{test}^{2}+\epsilon}}+\beta<br>\end{aligned}$</p>
<p>其中m表示的是批量大小。<br>这种做法有一个明显的缺点：需要消耗较大的存储空间，保存训练过程中所有的均值和方差结果（每一组，每一个，每一层）。</p>
<p><strong>（2）Momentum：移动平均法(Moving Average)</strong></p>
<p>稍微改变一下训练过程中计算均值和方差的办法，设 $\mu_t$是当前步骤求出的均值，  $\bar{\mu}$是之前的训练步骤累积起来求得的均值（也称running mean），则：<br>$\bar{\mu}\leftarrow p\bar{\mu}+(1-p)\mu^{t}$<br>其中，p是momentum的超参，表示模型在多大程度上依赖于过去的均值和当前的均值。 $\bar{\mu}$ 则是新一轮的ruuning mean，也就是当前步骤里最终使用的mean。同理，对于方差，我们也有：<br>$\bar{\sigma^{2}}\leftarrow p\bar{\sigma^{2}}+(1-p)\sigma^{2}{}^{t}$</p>
<p>采用这种方法的好处是：</p>
<ul>
<li>节省了存储空间，不需要保存所有的均值和方差结果，只需要保存running mean和running variance即可</li>
<li>方便在训练模型的阶段追踪模型的表现。一般来讲，在模型训练的中途，我们会塞入validation dataset，对模型训练的效果进行追踪。采用移动平均法，不需要等模型训练过程结束再去做无偏估计，我们直接用running mean和running variance就可以在validation上评估模型。</li>
</ul>
<h3 id="1-3-BN的优势总结"><a href="#1-3-BN的优势总结" class="headerlink" title="1.3 BN的优势总结"></a>1.3 BN的优势总结</h3><ul>
<li>通过解决ICS的问题，使得每一层神经网络的输入分布稳定，在这个基础上可以使用较大的学习率，加速了模型的训练速度</li>
<li>起到一定的正则作用，进而减少了dropout的使用。当我们通过BN规整数据的分布以后，就可以尽量避免一些极端值造成的overfitting的问题</li>
<li>使得数据不落入饱和性激活函数（如sigmoid，tanh等）饱和区间，避免梯度消失的问题</li>
</ul>
<h3 id="1-4-大反转：著名深度学习方法BN成功的秘密竟不在ICS？"><a href="#1-4-大反转：著名深度学习方法BN成功的秘密竟不在ICS？" class="headerlink" title="1.4 大反转：著名深度学习方法BN成功的秘密竟不在ICS？"></a><strong>1.4 大反转：著名深度学习方法BN成功的秘密竟不在ICS？</strong></h3><p>以解决ICS为目的而提出的BN，在各个比较实验中都取得了更优的结果。但是来自MIT的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1805.11604.pdf">Santurkar et al. 2019</a>却指出：</p>
<ul>
<li>就算发生了ICS问题，模型的表现也没有更差</li>
<li>BN对解决ICS问题的能力是有限的</li>
<li><strong>BN奏效的根本原因在于它让optimization landscape更平滑</strong></li>
</ul>
<p>而在这之后的很多论文里也都对这一点进行了不同理论和实验的论证。<br>（每篇论文的Intro部分开头总有一句话类似于：“BN的奏效至今还是个玄学”。。。）</p>
<p>图中是VGG网络在标准，BN，noisy-BN下的实验结果。其中noisy-BN表示对神经网络的每一层输入，都随机添加来自分布(non-zero mean, non-unit variance)的噪音数据，并且在不同的timestep上，这个分布的mean和variance都在改变。noisy-BN保证了在神经网络的每一层下，输入分布都有严重的ICS问题。但是从试验结果来看，noisy-BN的准确率比标准下的准确率还要更高一些，这说明ICS问题并不是模型效果差的一个绝对原因。</p>
<p><img src="https://pic2.zhimg.com/80/v2-6a413e04c09beebaa0bf72dd77e8ffa5_720w.webp" alt=""></p>
<p>而当用VGG网络训练CIFAR-10数据时，也可以发现，在更深层的网络（例如Layer11）中，在采用BN的情况下，数据分布也没有想象中的“规整”：</p>
<p><img src="https://pic3.zhimg.com/80/v2-66d16a7da62b23f809e3276f541178c6_720w.webp" alt=""></p>
<p>最后，在VGG网络上，对于不同的训练step，计算其在不同batch上loss和gradient的方差（a和b中的阴影部分），同时测量 $\quad\beta-smoothness$ （简答理解为l2-norm表示的在一个梯度下降过程中的最大斜率差）。可以发现BN相较于标准情况都来得更加平滑。</p>
<p><img src="https://pic3.zhimg.com/80/v2-0986effe9bd0b8211a2867bde46c7582_720w.webp" alt=""></p>
<h2 id="二、Layer-Normalization"><a href="#二、Layer-Normalization" class="headerlink" title="二、Layer Normalization"></a>二、Layer Normalization</h2><h3 id="2-1-背景"><a href="#2-1-背景" class="headerlink" title="2.1 背景"></a>2.1 背景</h3><p>BN提出后，被广泛作用在CNN任务上来处理图像，并取得了很好的效果。针对文本任务，<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1607.06450.pdf">Ba et al. 2016</a> 提出在RNN上使用Layer Normalization（以下简称LN）的方法，用于解决BN无法很好地处理文本数据长度不一的问题。例如采用RNN模型+BN，我们需要对不同数据的同一个位置的token向量计算 $\mu,\sigma^{2}$ ，在句子长短不一的情况下，容易出现：</p>
<ul>
<li>测试集中出现比训练集更长的数据，由于BN在训练中累积 $\mu_{batch},\sigma^2_{batch}$ ，在测试中使用累计的经验统计量的原因，导致测试集中多出来的数据没有相应的统计量以供使用。 （在实际应用中，通常会对语言类的数据设置一个max_len，多裁少pad，这时没有上面所说的这个问题。但这里我们讨论的是理论上的情况，即理论上，诸如Transformer这样的模型，是支持任意长度的输入数据的）</li>
<li>长短不一的情况下，文本中的某些位置没有足够的batch_size的数据，使得计算出来的  $\mu,\sigma^{2}$产生偏差。例如<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2003.07845.pdf">Shen et al. (2020)</a>就指出，在数据集Cifar-10（模型RestNet20)和IWLST14（模型Transformer）的训练过程中，计算当前epoch所有batch的统计量  $\mu_B,\sigma^{2}_B$和当前累计（running）统计量  $\mu,\sigma^{2}$ 的平均Euclidean distance，可以发现文本数据较图像数据的分布差异更大：</li>
</ul>
<p><img src="https://pic3.zhimg.com/80/v2-2c3498a034c43ff734e35fb5fe782902_720w.webp" alt=""></p>
<h3 id="2-2-思路"><a href="#2-2-思路" class="headerlink" title="2.2 思路"></a>2.2 思路</h3><p>整体做法类似于BN，不同的是LN不是在特征间进行标准化操作（横向操作），而是在整条数据间进行标准化操作（纵向操作）。</p>
<p><img src="https://pic4.zhimg.com/80/v2-839ecea737782bc8a8af321f2d4f8633_720w.webp" alt=""></p>
<p>在图像问题中，LN是指对一整张图片进行标准化处理，即在一张图片所有channel的pixel范围内计算均值和方差。而在NLP的问题中，LN是指在一个句子的一个token的范围内进行标准化。</p>
<p><img src="https://pic3.zhimg.com/80/v2-1c772d9cfebc50ea7bc89c188b15994a_720w.webp" alt=""></p>
<p><img src="https://pic2.zhimg.com/80/v2-6aff36a5399e1b50f5f220e8cb762861_720w.webp" alt=""></p>
<h2 id="三、Transformer-LN改进方法：Pre-LN"><a href="#三、Transformer-LN改进方法：Pre-LN" class="headerlink" title="三、Transformer LN改进方法：Pre-LN"></a>三、Transformer LN改进方法：Pre-LN</h2><h2 id="ResNet（残差网络）"><a href="#ResNet（残差网络）" class="headerlink" title="ResNet（残差网络）"></a>ResNet（残差网络）</h2><h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>在进行<strong>深层</strong>网络学习的过程中，有两个避不开的问题：</p>
<h3 id="1-1-梯度消失-爆炸"><a href="#1-1-梯度消失-爆炸" class="headerlink" title="1.1 梯度消失/爆炸"></a>1.1 梯度消失/爆炸</h3><p><img src="https://pic1.zhimg.com/80/v2-2efec6488291bf97f6f9bc55311afb0c_720w.webp" alt=""><em>上图是数据在神经网络中的传播过程。</em></p>
<p>如图所示的三层神经网络，每一层的线性层和非线性层可以表示为：</p>
<p>$\begin{aligned}&amp;z^{[L]}=W^{[L]}*A^{[L-1]}+b^{[L]}(\text{线性变化层})\\&amp;\mathrm{A}^{[L]}=g^{[L]}(Z^{[L]})(\text{非线性变化}/\text{激活函数层})\end{aligned}$</p>
<p>假设现在要计算第一层 W^[1]^ 的梯度，那么根据链式法则，有：</p>
<p>$\begin{gathered}<br>\frac{\partial LOSS}{\partial W^{[1]}} =\frac{\partial LOSS}{\partial A^{[3]}}\frac{\partial A^{[3]}}{\partial Z^{[3]}}\frac{\partial Z^{[3]}}{\partial A^{[2]}}\frac{\partial A^{[2]}}{\partial Z^{[2]}}\frac{\partial Z^{[2]}}{\partial A^{[1]}}\frac{\partial A^{[1]}}{\partial Z^{[1]}}\frac{\partial Z^{[1]}}{\partial W^{[1]}} \\<br>=\frac{\partial LOSS}{\partial A^{[3]}}g^{[3]}{}^{\prime}W^{[3]}g^{[2]}{}^{\prime}W^{[2]}g^{[1]}{}^{\prime}\frac{\partial Z^{[1]}}{\partial W^{[1]}}<br>\end{gathered}$</p>
<p>如果在神经网络中，多层都满足 $g^{[L]}{}^{\prime}W^{[L]}&gt;1$ ，则越往下的网络层的梯度越大，这就造成了<strong>梯度爆炸</strong>的问题。反之，若多层都满足 $g^{[L]}{}^{\prime}W^{[L]}&lt;1$ ，则越往下的网络层梯度越小，引起<strong>梯度消失</strong>的问题。而在深度学习网络中，为了让模型学到更多非线性的特征，在激活层往往使用例如<strong>sigmoid</strong>这样的激活函数。对sigmoid来说，<strong>其导数的取值范围在</strong> (0,$\frac14$] ，在层数堆叠的情况下，更容易出现梯度消失的问题。</p>
<p>面对梯度消失/爆炸的情况，可以通过Normalization等方式解决，使得模型最终能够收敛。</p>
<h3 id="1-2-网络退化-Degradation"><a href="#1-2-网络退化-Degradation" class="headerlink" title="1.2 网络退化(Degradation)"></a>1.2 网络退化(Degradation)</h3><p>因为梯度消失/爆炸所导致的深层网络模型不收敛的问题，已经得到了解决。那么现在新的问题出现了：<strong>在模型能够收敛的情况下，网络越深，模型的准确率越低，同时，模型的准确率先达到饱和，此后迅速下降</strong>。这个情况我们称之为<strong>网络退化（Degradation）。</strong>如下图，56层网络在测试集（右）上的错误率比20层网络要更高，这个现象也不是因为overfitting所引起的，因为在训练集上，深层网络的表现依然更差。</p>
<p><img src="https://pic1.zhimg.com/80/v2-d2d0726fcd67e946cee94e454e0e8a74_720w.webp" alt=""></p>
<p>因此，ResNet就作为一种解决网络退化问题的有效办法出现了，借助ResNet，我们能够有效训练出更深的网络模型（可以超过1000层），使得深网络的表现不差于浅网络。</p>
<h2 id="二、思路"><a href="#二、思路" class="headerlink" title="二、思路"></a>二、思路</h2><h3 id="2-1-为什么需要更深的网络"><a href="#2-1-为什么需要更深的网络" class="headerlink" title="2.1 为什么需要更深的网络"></a>2.1 为什么需要更深的网络</h3><p>神经网络帮我们避免了繁重的特征工程过程。借助神经网络中的非线形操作，可以帮助我们更好地拟合模型的特征。为了增加模型的表达能力，一种直觉的想法是，增加网络的深度，一来使得网络的每一层都尽量学到不同的模式，二来更好地利用网络的非线性拟合能力。</p>
<h3 id="2-2-理想中的深网络表现"><a href="#2-2-理想中的深网络表现" class="headerlink" title="2.2 理想中的深网络表现"></a>2.2 理想中的深网络表现</h3><p>理想中的深网络，其表现不应该差于浅网络。举一个简单的例子，下图左边是2层的浅网络，右边是4层的深网络，我们只要令深网络的最后两层的输入输出相等，那么两个网络就是等效的，这种操作被称为<strong>恒等映射（Identity Mapping)。</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-f5f85ded2b2e5b2e160381f866c380d0_720w.webp" alt=""></p>
<p>当然，这样完全相等的映射是一种极端情况，更为理想的情况是，在网络的深层，让网络尽量逼近这样的极端情况，使得网络在学到新东西的同时，其输出又能逼近输入，这样就能保证深网络的效果不会比浅网络更差。</p>
<p><strong>总结：在网络的深层，需要学习一种恒等映射（Identity Mapping）。</strong></p>
<h2 id="三、实践和实验效果"><a href="#三、实践和实验效果" class="headerlink" title="三、实践和实验效果"></a>三、实践和实验效果</h2><h3 id="3-1-构造恒等映射：残差学习（residule-learning）"><a href="#3-1-构造恒等映射：残差学习（residule-learning）" class="headerlink" title="3.1 构造恒等映射：残差学习（residule learning）"></a>3.1 构造恒等映射：残差学习（residule learning）</h3><p>最暴力的构造恒等映射的方法，就是在相应网络部分的尾端增加一层学习层 W^[IM]^ ，来满足输出和输入逼近。但是本来深网络要学的参数就很庞大了，再构造新的参数层，又增加了模型的复杂度。</p>
<p><img src="https://pic4.zhimg.com/80/v2-0ac8ea1d91caf249bd81fa98e20a14fb_720w.webp" alt=""></p>
<p>能不能在不添加参数层的情况下，实现恒等映射的功能？考虑下图：</p>
<p><img src="https://pic3.zhimg.com/80/v2-7d39c91638d5c21d6abb63b666c799ea_720w.webp" alt=""></p>
<p>蓝色星星 f∗ 是“真正”拟合我们数据集的函数，而 f1,…,f6 表示分别表示不同层数的神经网络（层数为1的，层数为2的…）。在左边的构造方式中，函数是非嵌套的，可以发现6层神经网络可能比单层神经网络距离最优解更远。而在右边的嵌套式构造中，则保证了更多层的神经网络至少能取到更浅的神经网络的最优解。</p>
<blockquote>
<p>什么是嵌套函数？</p>
<p>嵌套函数是完全包含在父函数内的函数。程序文件中的任何函数都可以包含嵌套函数。</p>
<p>嵌套函数与其他类型的函数的主要区别是，嵌套函数可以访问和修改在其父函数中定义的变量。因此：</p>
<ul>
<li>嵌套函数可以使用不是以输入参数形式显式传递的变量。</li>
<li>在父函数中，您可以为嵌套函数创建包含运行嵌套函数所必需的数据的句柄。</li>
</ul>
<p>from :<a target="_blank" rel="noopener" href="https://ww2.mathworks.cn/help/matlab/matlab_prog/nested-functions.html">嵌套函数 - MATLAB &amp; Simulink </a></p>
</blockquote>
<p>受到这一思想的启发，我们在深层网络中引入残差模块，具体运作方式如下：</p>
<p><img src="https://pic4.zhimg.com/80/v2-9f799e7602038a0ac02707517638bf83_720w.webp" alt=""></p>
<p>如图所示，这个残差模块包含了神经网络中的两层，其中， X 表示输入， F(X) 表示过这两层之后的结果， H(X) 表示恒等映射，则在这样的构造方式下，恒等映射可以写成：</p>
<p>$\mathcal{H}(X)=\mathcal{F}(X)+X$</p>
<p>F(X) 就被称之为<strong>残差函数（residule function）。</strong>在网络深层的时候，在优化目标的约束下，模型通过学习使得 F(X) 逼近0<strong>（residule learning)</strong>，让深层函数在学到东西的情况下，又不会发生网络退化的问题。</p>
<h3 id="3-2-实验过程及结果"><a href="#3-2-实验过程及结果" class="headerlink" title="3.2 实验过程及结果"></a>3.2 实验过程及结果</h3><p>在ResNet的论文中，做了很丰富的实验，这里仅贴出它在ImageNet 2012数据集上的试验结果。<br>这个实验的网络以VGG网络为参考，构造了<strong>34-layer plain DNN</strong>和<strong>34-layer residule DNN。</strong>前者是一个标准的深层网络，后者是增加残差处理的深层网络。基本构造如下：（图很小吧，都怪网络太深，没关系，不看也可以）</p>
<p><img src="https://pic3.zhimg.com/80/v2-75f8e8f832a4b7124bc9f2c4fcd42b72_720w.webp" alt=""></p>
<p>以下是实验结果。左图是18层和34层的plain DNN，右是18层和34层的residule DNN。粗线表示训练集上的错误率，细线表示验证集上的错误率。可以发现在残差网络中，深网络的错误率都已经被压到了浅网络之下，同时也比plain DNN的错误率更低。</p>
<p><img src="https://pic1.zhimg.com/80/v2-3f9ae06e5aac7ced5ae00b77e665e174_720w.webp" alt=""></p>
<p><img src="https://pic2.zhimg.com/80/v2-8fd621229760621635b1969f7120774d_720w.webp" alt=""></p>
<h2 id="四、Transformer中的残差连接"><a href="#四、Transformer中的残差连接" class="headerlink" title="四、Transformer中的残差连接"></a>四、Transformer中的残差连接</h2><p>在transformer的encoder和decoder中，各用到了6层的attention模块，每一个attention模块又和一个FeedForward层（简称FFN）相接。对每一层的attention和FFN，都采用了一次残差连接，即把每一个位置的输入数据和输出数据相加，使得Transformer能够有效训练更深的网络。在残差连接过后，再采取Layer Nomalization的方式。具体的操作过程见下图，箭头表示画不下了，从左边转到右边去画：</p>
<p><img src="https://pic4.zhimg.com/80/v2-615e73c1a94b944e7aefbdc663176f47_720w.webp" alt=""></p>
<h2 id="Subword-Tokenization（子词分词器）"><a href="#Subword-Tokenization（子词分词器）" class="headerlink" title="Subword Tokenization（子词分词器）"></a>Subword Tokenization（子词分词器）</h2><h2 id="一、分词器的组成"><a href="#一、分词器的组成" class="headerlink" title="一、分词器的组成"></a>一、分词器的组成</h2><p>如前所说，分词器的作用是将预料拆分成能入模的token。</p>
<p>分词器一般包含以下几个部分：</p>
<ul>
<li><strong>Trainer（分词训练器/算法）</strong>：给定训练数据，用Trainer封装的分词算法，生成词表</li>
<li><strong>Vocabulary（词表）</strong>：词表是分词的标准。基本上每个语言任务都需要维护一个词表，词表可以是根据当前训练数据生成的，也可以直接加载前人预训练好的词表。</li>
<li><strong>Encoder（编码器）</strong>：拥有词表后，给定语料，编码器把语料按词表拆分token，并记录token在词表中的索引，拥有该索引则可将token表示成one-hot向量入模，再做后续的embedding等优化操作。</li>
<li><strong>Decoder（解码器）</strong>：给定一串索引，将索引还原成最初的语料形式。</li>
</ul>
<p>我们用HuggingFace中提供的Bert分词器做一个例子，在Bert中使用分词算法是WordPiece（在这篇笔记里会介绍）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">sequence = <span class="string">&quot;A Titan RTX has 24GB of VRAM&quot;</span></span><br><span class="line"></span><br><span class="line">tokenized_sequence = tokenizer.tokenize(sequence) <span class="comment"># 用定义的分词器对语料分词</span></span><br><span class="line"><span class="built_in">print</span>(tokenized_sequence)</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;Titan&#x27;</span>, <span class="string">&#x27;R&#x27;</span>, <span class="string">&#x27;##T&#x27;</span>, <span class="string">&#x27;##X&#x27;</span>, <span class="string">&#x27;has&#x27;</span>, <span class="string">&#x27;24&#x27;</span>, <span class="string">&#x27;##GB&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;V&#x27;</span>, <span class="string">&#x27;##RA&#x27;</span>, <span class="string">&#x27;##M&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>分词器将语料拆分为多个token。以<code>&#39;R&#39;, &#39;##T&#39;, &#39;##X&#39;</code>为例，这里是分词器在说：“我的词表里没有<code>RTX</code>这个词，只有<code>R,T,X</code>三个字母，因此我只好把它们按词表拆开。但我知道<code>RTX</code>代表一个我不认识的整体，因此我用<code>##</code>把它们连接起来，团魂不灭！</p>
<p>我们也可以一步到位，把上面的文字token转变为在词表中的索引，其中101，102是Bert中输入的特殊标志符号<code>CLS</code>和<code>SEP</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer(sequence)</span><br><span class="line">encoded_sequence = inputs[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line"><span class="built_in">print</span>(encoded_sequence)</span><br><span class="line">[<span class="number">101</span>, <span class="number">138</span>, <span class="number">18696</span>, <span class="number">155</span>, <span class="number">1942</span>, <span class="number">3190</span>, <span class="number">1144</span>, <span class="number">1572</span>, <span class="number">13745</span>, <span class="number">1104</span>, <span class="number">159</span>, <span class="number">9664</span>, <span class="number">2107</span>, <span class="number">102</span>]</span><br><span class="line"></span><br><span class="line">decoded_sequence = tokenizer.decode(encoded_sequence)</span><br><span class="line"><span class="built_in">print</span>(decoded_sequence)</span><br><span class="line">[CLS] A Titan RTX has 24GB of VRAM [SEP] </span><br></pre></td></tr></table></figure>
<h2 id="二、分词器的类别"><a href="#二、分词器的类别" class="headerlink" title="二、分词器的类别"></a>二、分词器的类别</h2><h3 id="2-1-Word-level：基于空格的分词器"><a href="#2-1-Word-level：基于空格的分词器" class="headerlink" title="2.1 Word-level：基于空格的分词器"></a>2.1 Word-level：基于空格的分词器</h3><p>早期的做法，是按空格拆分单词，将一个单词做为一个token纳入词表，因此也说是word-level维度的。若语料里出现不在词表中的token，也称<strong>OOV(Out Of Vocabulary)</strong>，则此时常用<code>&lt;UNK&gt;</code>(Unknown)这个特殊符号来代替。这个分词器存在以下几个问题：</p>
<ul>
<li>大词表问题。词表大小和一门语言词汇量大小密切相关，当词表太大时，一来增加模型的存储负担。二来，<strong>token的one-hot向量也会非常长，增加后续embedding的训练参数</strong>。</li>
<li>罕见词难表示。</li>
<li>词汇冗余。例如love, loving, loved其实拥有相似的含义，但在词表中作为独立词存在。</li>
</ul>
<h3 id="2-2-Character-level：基于字符的分词器"><a href="#2-2-Character-level：基于字符的分词器" class="headerlink" title="2.2 Character-level：基于字符的分词器"></a>2.2 Character-level：基于字符的分词器</h3><p>每个字符作为一个词。例如英语中只有26个字符，那词表大小就只有26个。但这个level上依然存在问题：</p>
<ul>
<li>单个字符本身缺少语义。</li>
<li>输入序列变长。从原来的一句话变成若干个字符</li>
</ul>
<h3 id="2-3-Subword-level：基于子词的分词器"><a href="#2-3-Subword-level：基于子词的分词器" class="headerlink" title="2.3 Subword-level：基于子词的分词器"></a>2.3 Subword-level：基于子词的分词器</h3><p>子词分词器类似于借助词根词源来学习一系列单词。例如<code>transformer = trans + form + er</code>，<code>transfer = trans + fer</code>。借助这些共通的基本单元，就可以构造更多的词汇。每一个单元就是词表的组成。当然，我们的分词器是基于统计意义的分词器，不是基于语言学的。只是当它看过大量训练数据以后，它能学习到语言学上的含义。主要的子词分词器有三类：BPE，WordPiece和ULM。</p>
<h2 id="三、Byte-Pair-Encoding-BPE"><a href="#三、Byte-Pair-Encoding-BPE" class="headerlink" title="三、Byte Pair Encoding (BPE)"></a>三、Byte Pair Encoding (BPE)</h2><p>Transformer中，在对WMT 2014 English-German数据集进行处理的时候，就用到了BPE。Sennrich et al. 2015最早将BPE用在Neural Machine Translation领域上，这个算法是目前最流行的NLP分词方法之一（GPT-2采用的也是该方法）。</p>
<h3 id="3-1-BPE的训练步骤（Trainer）"><a href="#3-1-BPE的训练步骤（Trainer）" class="headerlink" title="3.1 BPE的训练步骤（Trainer）"></a>3.1 BPE的训练步骤（Trainer）</h3><ul>
<li>收集训练语料，并确定期望的subword大小，也就是词表大小</li>
<li>将语料中每一个单词的末尾添上特殊字符<code>&lt;/w&gt;</code>，该字符表示单词的结束。举例来说，同样是<code>er</code>，它在<code>lower</code>和在<code>era</code>中的意义就不一样，这就是为什么要加特殊字符。</li>
<li>统计单词在整个训练语料中的出现频率。例如单词low在训练语料中出现了5次，则记为<code>&#39;low&lt;/w&gt;&#39;: 5</code></li>
<li>把单词拆分成字符，即<code>&#39;low&lt;/w&gt;&#39;: 5</code>变成<code>l o w &lt;/w&gt;&#39;: 5</code></li>
<li>统计所有连续字符对的出现频率。假设我们有<code>l o w &lt;/w&gt;&#39;: 5</code>和<code>&#39;h o w &lt;/w&gt;&#39;: 3</code>，则此时有<code>(&#39;l&#39;, &#39;o&#39;): 5, (&#39;o&#39;, &#39;w&#39;): 5+3 = 8, (&#39;h&#39;, &#39;o&#39;): 3</code>。取出现频率最高的字符对将它们合并。例如<code>(&#39;o&#39;, &#39;w&#39;)</code>出现频率最高，则此时变成<code>&#39;l ow&lt;/w&gt;&#39;: 5, &#39;h ow&lt;/w&gt;&#39;:3</code>，此时的词表为<code>&#39;l&#39;, &#39;ow&#39;, &#39;h&#39;</code>。</li>
<li>重复上一步，直到词表达到预定的大小，或者所有连续字符对的最高出现频率为1</li>
</ul>
<p>沿用Sennrich et al. 2015在论文中给出的例子<code>‘low&lt;/w&gt;’:5 ‘lower&lt;/w&gt;’:2 ‘newest&lt;/w&gt;’:6 ‘widest&lt;/w&gt;’:3</code>，这里展示一个demo来看下算法运作的流程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_stats</span>(<span class="params">vocab: <span class="built_in">dict</span></span>)-&gt;<span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从当前训练数据中，计算所有连续字符对出现的频率</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab: 当前训练数据，key为当前每个单词的形式，value为对应频率</span></span><br><span class="line"><span class="string">               形式诸如：&#123;&#x27;l o w &lt;/w&gt;&#x27;: 5, &#x27;l o w e r &lt;/w&gt;&#x27;: 2, &#x27;n e w e s t &lt;/w&gt;&#x27;: 6, &#x27;w i d e s t &lt;/w&gt;&#x27;: 3&#125;</span></span><br><span class="line"><span class="string">    Return</span></span><br><span class="line"><span class="string">        pairs：所有连续字符对出现的频率，key为连续字符对，value为频率</span></span><br><span class="line"><span class="string">               形式诸如：&#123;(&#x27;l&#x27;, &#x27;o&#x27;): 7, (&#x27;o&#x27;, &#x27;w&#x27;): 7, (&#x27;w&#x27;, &#x27;&lt;/w&gt;&#x27;): 5, </span></span><br><span class="line"><span class="string">                        (&#x27;w&#x27;, &#x27;e&#x27;): 8, (&#x27;e&#x27;, &#x27;r&#x27;): 2, (&#x27;r&#x27;, &#x27;&lt;/w&gt;&#x27;): 2, </span></span><br><span class="line"><span class="string">                        (&#x27;n&#x27;, &#x27;e&#x27;): 6, (&#x27;e&#x27;, &#x27;w&#x27;): 6, (&#x27;e&#x27;, &#x27;s&#x27;): 9, </span></span><br><span class="line"><span class="string">                        (&#x27;s&#x27;, &#x27;t&#x27;): 9, (&#x27;t&#x27;, &#x27;&lt;/w&gt;&#x27;): 9, (&#x27;w&#x27;, &#x27;i&#x27;): 3, </span></span><br><span class="line"><span class="string">                        (&#x27;i&#x27;, &#x27;d&#x27;): 3, (&#x27;d&#x27;, &#x27;e&#x27;): 3&#125;)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pairs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(symbols)-<span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i],symbols[i+<span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge_vocab</span>(<span class="params">pair:<span class="built_in">tuple</span>, v_in:<span class="built_in">tuple</span></span>)-&gt; <span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用找出的频率最大的字符对（pair）去对当前训练数据做合并</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pair: 当前频率最大的子词对，形式诸如(&#x27;e&#x27;, &#x27;s&#x27;)</span></span><br><span class="line"><span class="string">        v_in: 当前训练数据</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        v_out: 合并结果。例如用&#x27;es&#x27;替换&#x27;e s&#x27;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">&#x27; &#x27;</span>.join(pair)) <span class="comment"># 对字符串中所有可能被解释为正则运算的字符进行转义。例如&#x27;e s&#x27;变为&#x27;e\ s&#x27;</span></span><br><span class="line">    p = re.<span class="built_in">compile</span>(<span class="string">r&#x27;(?&lt;!\S)&#x27;</span> + bigram + <span class="string">r&#x27;(?!\S)&#x27;</span>) <span class="comment"># 创建正则匹配对象</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">&#x27;&#x27;</span>.join(pair), word) <span class="comment"># 对word中满足正则匹配的部分，用&#x27;&#x27;.join(pair)代替</span></span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">len_token</span>(<span class="params">token: <span class="built_in">str</span></span>)-&gt;<span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算一个token的长度。主要是将特殊标记&#x27;&lt;/w&gt;&#x27;当成一个字符来处理</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token[-<span class="number">4</span>:] == <span class="string">&#x27;&lt;/w&gt;&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(token[:-<span class="number">4</span>]) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(token)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_tokens</span>(<span class="params">vocab:<span class="built_in">dict</span></span>)-&gt;<span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    根据当前训练数据生成当下的bpe词表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    bpe_vocab = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        <span class="keyword">for</span> symbol <span class="keyword">in</span> word.split():</span><br><span class="line">            bpe_vocab[symbol] += freq</span><br><span class="line">    bpe_vocab = <span class="built_in">dict</span>(<span class="built_in">sorted</span>(bpe_vocab.items(), key=<span class="keyword">lambda</span> x: len_token(x[<span class="number">0</span>]), reverse = <span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bpe_vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_bpe_vocab</span>(<span class="params">v_in:<span class="built_in">dict</span>, num_merges: <span class="built_in">int</span></span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    生成bpe词表</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        v_in: 原始训练数据</span></span><br><span class="line"><span class="string">        num_merges: 合并次数，也是预设的最终的词表大小</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        bpe_vocab: bpe词表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    v_out = v_in.copy()</span><br><span class="line">    bpe_vocab = get_tokens(v_in)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;执行BPE前：&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;训练数据为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(v_in))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;初始化词表为: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(bpe_vocab.keys()))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;初始化词表长度为: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(bpe_vocab)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span> * <span class="number">50</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Iter: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">        pairs = get_stats(v_out)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            best = <span class="built_in">max</span>(pairs, key = pairs.get)</span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;所有词汇已不可拆分，停止生成词表&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> pairs[best] &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;字符对出现频率小于2，停止生成词表&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        v_out = merge_vocab(best, v_out)</span><br><span class="line">        bpe_vocab = get_tokens(v_out)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;当前Iter出现频率最大的连续字符对为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(best))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;当前Iter合并后的新训练数据为: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(v_out))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;当前Iter新生成的词表为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(bpe_vocab.keys()))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;当前Iter新生成的词表长度为: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(bpe_vocab)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bpe_vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    vocab = &#123;<span class="string">&#x27;l o w &lt;/w&gt;&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;l o w e r &lt;/w&gt;&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;n e w e s t &lt;/w&gt;&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;w i d e s t &lt;/w&gt;&#x27;</span>: <span class="number">3</span>&#125;</span><br><span class="line">    num_merges = <span class="number">7</span></span><br><span class="line">    bpe_tokens = generate_bpe_vocab(vocab, num_merges)</span><br></pre></td></tr></table></figure>
<h3 id="3-2-BPE的编码步骤（Encoder）"><a href="#3-2-BPE的编码步骤（Encoder）" class="headerlink" title="3.2 BPE的编码步骤（Encoder）"></a>3.2 BPE的编码步骤（Encoder）</h3><p>训练步骤结束后，我们得到一个词表，Encoder阶段要解决的事情，就是给定一个语料，基于词表对该语料进行分词，具体步骤如下：</p>
<ul>
<li>将词表中的token按长度从长到短进行排序（上述代码已经实现了这一点）</li>
<li>对语料中的每一个单词，遍历排序好的词表，查看当前的token是否是该单词的子字符串。如果是，则输出当前token，并对剩余的字符串继续匹配。</li>
<li>当遍历完一次词表过后，如果仍有字符串在词表中找不到匹配，就把这些字符串用特殊符号输出，比如<code>&lt;UNK&gt;</code></li>
</ul>
<p>以上述代码生成的词表<code>[&#39;est&lt;/w&gt;&#39;, &#39;low&#39;, &#39;new&#39;, &#39;&lt;/w&gt;&#39;, &#39;e&#39;, &#39;r&#39;, &#39;w&#39;, &#39;i&#39;, &#39;d&#39;]</code>为例，假设有一单词为<code>hiest</code>，则该单词最终将被切分为<code>&lt;UNK&gt; i est&lt;/w&gt;</code>。</p>
<h3 id="3-3-BPE的解码步骤（Decoder）"><a href="#3-3-BPE的解码步骤（Decoder）" class="headerlink" title="3.3 BPE的解码步骤（Decoder）"></a>3.3 BPE的解码步骤（Decoder）</h3><p>当相邻的token间没有特殊符号（例如空格/终止符），则表示这两个token同属一个单词，将它们直接拼接即可。否则需要在两个token间添加分隔符。</p>
<h2 id="四、WordPiece"><a href="#四、WordPiece" class="headerlink" title="四、WordPiece"></a>四、WordPiece</h2><p>Bert模型在做分词的时候用的就是WordPiece方法。WordPiece方法和BPE总体很相似，不同的是，在选择合并字符时，BPE采用的标准是“频率最高”，而WordPiece则是“概率最大”。具体来说，WordPiece在每次合并时将选择一对相邻的子字符，这对子字符满足：当合并它们时，它们对语料库语言模型的概率提升最大。</p>
<p>训练暂略。</p>
<h2 id="五、Unigram-Language-Model-ULM"><a href="#五、Unigram-Language-Model-ULM" class="headerlink" title="五、Unigram Language Model (ULM)"></a>五、Unigram Language Model (ULM)</h2><h2 id="五、Unigram-Language-Model-ULM-1"><a href="#五、Unigram-Language-Model-ULM-1" class="headerlink" title="五、Unigram Language Model (ULM)"></a>五、Unigram Language Model (ULM)</h2><p>总结BPE和WordPiece，我们发现它们都是通过一定的方法，生成一个最终的词表，然后对于某个单词或句子，根据这个最终的词表，生成<strong>唯一</strong>的划分结果。</p>
<p>但是，<strong>这个划分结果一定就是最好的吗</strong>？假设通过BPE或WordPiece，生成了一张词表<code>est&lt;/w&gt;, st&lt;/w&gt;, w, e, s, t</code>，那么对于单词<code>west</code>，它有三种划分方式：<code>w est&lt;/w&gt;</code>，<code>w e st&lt;/w&gt;</code>和<code>w e s t</code>。根据前两个算法的编码规则，我们选择了第一个，但是也许在后续做训练任务时，其实第三个的表现更好呢？</p>
<p>因此，ULM就作为一种改进办法出现了。概括来说，它的改进思路是：</p>
<ul>
<li>首先拥有一种大词表</li>
<li>基于这张词表，对所有语料的所有子词划分结果做一个总体评分。然后从词表中<strong>剔除掉那些对总体评分贡献最小的子词</strong></li>
<li>重复第二步，不断丢弃贡献度小的子词，直到词表达到预设大小</li>
<li>生成最终的词表。基于该词表，一个语料仍可能有多种划分，通过某种方式，选择其中最优的划分。</li>
</ul>
<p>算法暂略。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kokomisukisuki.github.io">sagonimikakokomi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kokomisukisuki.github.io/2023/11/18/transformer/">https://kokomisukisuki.github.io/2023/11/18/transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kokomisukisuki.github.io" target="_blank">Hello</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post_share"><div class="social-share" data-image="https://www.helloimg.com/images/2023/10/03/oHX5TE.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/11/18/ChatGPT%E6%8A%80%E6%9C%AF/" title=""><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2023/11/18/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%AD%A5/" title="强化学习"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oH3BLh.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">强化学习</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/11/18/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%AD%A5/" title="强化学习"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oH3BLh.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-18</div><div class="title">强化学习</div></div></a></div><div><a href="/2023/11/18/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%EF%BC%88stable%20diffusion%EF%BC%89/" title="DDPM扩散模型（stable diffusion）"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oHJc2r.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-18</div><div class="title">DDPM扩散模型（stable diffusion）</div></div></a></div><div><a href="/2023/11/18/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%EF%BC%88GAN%EF%BC%89/" title="生成对抗网络（GAN）"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oH3BLh.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-18</div><div class="title">生成对抗网络（GAN）</div></div></a></div><div><a href="/2023/11/18/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB/" title="马尔可夫"><img class="cover" src="https://www.helloimg.com/images/2023/10/03/oH3yev.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-18</div><div class="title">马尔可夫</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer"><span class="toc-text">transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E6%98%93%E6%A8%A1%E5%9E%8B"><span class="toc-text">简易模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%E5%BC%A0%E9%87%8F"><span class="toc-text">原理张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E5%85%A5%E7%BC%96%E7%A0%81"><span class="toc-text">进入编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%B0%B4%E5%B9%B3%E8%87%AA%E6%B3%A8%E6%84%8F"><span class="toc-text">高水平自注意</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E7%9A%84%E7%BB%86%E8%8A%82"><span class="toc-text">自注意的细节</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97"><span class="toc-text">自注意矩阵计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E7%9D%80%E8%AE%B8%E5%A4%9A%E5%A4%B4%E7%9A%84%E9%87%8E%E5%85%BD"><span class="toc-text">长着许多头的野兽</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E8%A1%A8%E7%A4%BA%E5%BA%8F%E5%88%97%E7%9A%84%E9%A1%BA%E5%BA%8F"><span class="toc-text">使用位置编码表示序列的顺序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE"><span class="toc-text">残差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E7%AB%AF"><span class="toc-text">解码器端</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E7%BB%88%E7%9A%84%E7%BA%BF%E6%80%A7%E5%B1%82%E5%92%8C-Softmax-%E5%B1%82"><span class="toc-text">最终的线性层和 Softmax 层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%9B%9E%E9%A1%BE"><span class="toc-text">训练回顾</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB"><span class="toc-text">详细解读</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-text">一、为什么使用位置编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%9E%84%E9%80%A0%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E6%96%B9%E6%B3%95-%E6%BC%94%E5%8F%98%E5%8E%86%E7%A8%8B"><span class="toc-text">二、构造位置编码的方法 &#x2F;演变历程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E7%94%A8%E6%95%B4%E5%9E%8B%E5%80%BC%E6%A0%87%E8%AE%B0%E4%BD%8D%E7%BD%AE"><span class="toc-text">2.1 用整型值标记位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%94%A8-0-1-%E8%8C%83%E5%9B%B4%E6%A0%87%E8%AE%B0%E4%BD%8D%E7%BD%AE"><span class="toc-text">2.2 用[0,1]范围标记位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E7%94%A8%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%90%91%E9%87%8F%E6%A0%87%E8%AE%B0%E4%BD%8D%E7%BD%AE"><span class="toc-text">2.3 用二进制向量标记位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E7%94%A8%E5%91%A8%E6%9C%9F%E5%87%BD%E6%95%B0%EF%BC%88sin%EF%BC%89%E6%9D%A5%E8%A1%A8%E7%A4%BA%E4%BD%8D%E7%BD%AE"><span class="toc-text">2.4 用周期函数（sin）来表示位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E7%94%A8sin%E5%92%8Ccos%E4%BA%A4%E6%9B%BF%E6%9D%A5%E8%A1%A8%E7%A4%BA%E4%BD%8D%E7%BD%AE"><span class="toc-text">2.5 用sin和cos交替来表示位置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81Transformer%E4%B8%AD%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%96%B9%E6%B3%95%EF%BC%9ASinusoidal-functions"><span class="toc-text">三、Transformer中位置编码方法：Sinusoidal functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Transformer-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%AE%9A%E4%B9%89"><span class="toc-text">3.1 Transformer 位置编码定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Transformer%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">3.2 Transformer位置编码可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Transformer%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E8%B4%A8"><span class="toc-text">3.3 Transformer位置编码的重要性质</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F"><span class="toc-text">自注意</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%9F%BA%E4%BA%8E%E7%BB%93%E6%9E%84"><span class="toc-text">一、基于结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Attention%E6%9E%84%E9%80%A0"><span class="toc-text">二、Attention构造</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Attention%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%BF%90%E4%BD%9C%E6%96%B9%E5%BC%8F"><span class="toc-text">2.1 Attention的基本运作方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Attention%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E5%9B%BE%E8%A7%A3"><span class="toc-text">2.2 Attention的计算过程图解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Masked-Attention"><span class="toc-text">2.3 Masked Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Multihead-Attention"><span class="toc-text">2.4 Multihead Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81Attention%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5"><span class="toc-text">三、Attention代码实践</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E4%BB%A5%E5%8F%8A%E5%B1%82%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-text">批量以及层标准化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81Batch-Normalization"><span class="toc-text">一、Batch Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%8F%90%E5%87%BA%E8%83%8C%E6%99%AF"><span class="toc-text">1.1 提出背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-1-ICS%E6%89%80%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">1.1.1 ICS所带来的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-2-%E8%A7%A3%E5%86%B3ICS%E7%9A%84%E5%B8%B8%E8%A7%84%E6%96%B9%E6%B3%95"><span class="toc-text">1.1.2 解决ICS的常规方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-BN%E7%9A%84%E5%AE%9E%E8%B7%B5"><span class="toc-text">1.2 BN的实践</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-%E6%80%9D%E8%B7%AF"><span class="toc-text">1.2.1 思路</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84BN"><span class="toc-text">1.2.1 训练过程中的BN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-2-%E6%B5%8B%E8%AF%95%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84BN"><span class="toc-text">1.2.2 测试过程中的BN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-BN%E7%9A%84%E4%BC%98%E5%8A%BF%E6%80%BB%E7%BB%93"><span class="toc-text">1.3 BN的优势总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E5%A4%A7%E5%8F%8D%E8%BD%AC%EF%BC%9A%E8%91%97%E5%90%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95BN%E6%88%90%E5%8A%9F%E7%9A%84%E7%A7%98%E5%AF%86%E7%AB%9F%E4%B8%8D%E5%9C%A8ICS%EF%BC%9F"><span class="toc-text">1.4 大反转：著名深度学习方法BN成功的秘密竟不在ICS？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Layer-Normalization"><span class="toc-text">二、Layer Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%83%8C%E6%99%AF"><span class="toc-text">2.1 背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%80%9D%E8%B7%AF"><span class="toc-text">2.2 思路</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81Transformer-LN%E6%94%B9%E8%BF%9B%E6%96%B9%E6%B3%95%EF%BC%9APre-LN"><span class="toc-text">三、Transformer LN改进方法：Pre-LN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet%EF%BC%88%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%EF%BC%89"><span class="toc-text">ResNet（残差网络）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%83%8C%E6%99%AF"><span class="toc-text">一、背景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-%E7%88%86%E7%82%B8"><span class="toc-text">1.1 梯度消失&#x2F;爆炸</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%BD%91%E7%BB%9C%E9%80%80%E5%8C%96-Degradation"><span class="toc-text">1.2 网络退化(Degradation)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%80%9D%E8%B7%AF"><span class="toc-text">二、思路</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%9B%B4%E6%B7%B1%E7%9A%84%E7%BD%91%E7%BB%9C"><span class="toc-text">2.1 为什么需要更深的网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%90%86%E6%83%B3%E4%B8%AD%E7%9A%84%E6%B7%B1%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%8E%B0"><span class="toc-text">2.2 理想中的深网络表现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%9E%E8%B7%B5%E5%92%8C%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C"><span class="toc-text">三、实践和实验效果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%9E%84%E9%80%A0%E6%81%92%E7%AD%89%E6%98%A0%E5%B0%84%EF%BC%9A%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0%EF%BC%88residule-learning%EF%BC%89"><span class="toc-text">3.1 构造恒等映射：残差学习（residule learning）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%AE%9E%E9%AA%8C%E8%BF%87%E7%A8%8B%E5%8F%8A%E7%BB%93%E6%9E%9C"><span class="toc-text">3.2 实验过程及结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Transformer%E4%B8%AD%E7%9A%84%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5"><span class="toc-text">四、Transformer中的残差连接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Subword-Tokenization%EF%BC%88%E5%AD%90%E8%AF%8D%E5%88%86%E8%AF%8D%E5%99%A8%EF%BC%89"><span class="toc-text">Subword Tokenization（子词分词器）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%88%86%E8%AF%8D%E5%99%A8%E7%9A%84%E7%BB%84%E6%88%90"><span class="toc-text">一、分词器的组成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%88%86%E8%AF%8D%E5%99%A8%E7%9A%84%E7%B1%BB%E5%88%AB"><span class="toc-text">二、分词器的类别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Word-level%EF%BC%9A%E5%9F%BA%E4%BA%8E%E7%A9%BA%E6%A0%BC%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">2.1 Word-level：基于空格的分词器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Character-level%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%AD%97%E7%AC%A6%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">2.2 Character-level：基于字符的分词器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Subword-level%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%AD%90%E8%AF%8D%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text">2.3 Subword-level：基于子词的分词器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81Byte-Pair-Encoding-BPE"><span class="toc-text">三、Byte Pair Encoding (BPE)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-BPE%E7%9A%84%E8%AE%AD%E7%BB%83%E6%AD%A5%E9%AA%A4%EF%BC%88Trainer%EF%BC%89"><span class="toc-text">3.1 BPE的训练步骤（Trainer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-BPE%E7%9A%84%E7%BC%96%E7%A0%81%E6%AD%A5%E9%AA%A4%EF%BC%88Encoder%EF%BC%89"><span class="toc-text">3.2 BPE的编码步骤（Encoder）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-BPE%E7%9A%84%E8%A7%A3%E7%A0%81%E6%AD%A5%E9%AA%A4%EF%BC%88Decoder%EF%BC%89"><span class="toc-text">3.3 BPE的解码步骤（Decoder）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81WordPiece"><span class="toc-text">四、WordPiece</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81Unigram-Language-Model-ULM"><span class="toc-text">五、Unigram Language Model (ULM)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81Unigram-Language-Model-ULM-1"><span class="toc-text">五、Unigram Language Model (ULM)</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By sagonimikakokomi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">welcome</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async defer src='/js/diy.js'></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div></body></html>